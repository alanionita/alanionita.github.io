<?xml version="1.0" encoding="UTF-8"?>
        <rss version="2.0" xmlns:content="http://purl.org/rss/1.0/modules/content/">
            <channel>
                <title>Alan Ionita - Fullstack developer porfolio, React, AWS, Typescript, bun</title>
                <link>https://alanionita.github.io/portfolio--svelte</link>
                <description>Latest blog posts</description>
                <language>en-GB</language>
                <lastBuildDate>Tue, 02 Sep 2025 14:20:41 GMT</lastBuildDate>
                <item>
            <title>Complex Amplify VTL resolver</title>
            <link>https://alanionita.github.io/portfolio--svelte/blog/2025.09.02.complex-amplify-vtl-writing-testing</link>
            <pubDate>Tue, 02 Sep 2025 00:00:00 +0100</pubDate>
            <author>Alan Ionita</author>
            <description>
                <![CDATA[
                    <h1>Complex Amplify VTL resolver writing and testing</h1>
<p>Let's start with a problem: you want to implement a profile editing feature, your app is a VueJS frontend and an Node AppSync GraphQL backend, and you database layer is a set of DynamoDB tables.
As for the implementation, the AppSync resolvers are a mix of Javascript Lambdas and Apache VTL resolvers.</p>
<p>The profile editing requires dynamic data inputs: when editing a profile you can edit one feature, none of them, or all of them.</p>
<p>Here's a simple DynamoDB.UpdateItem command example.</p>
<pre><code class="language-vtl">{
    "version" : "2018-05-29",
    "operation" : "UpdateItem",
    "key": {
        "id" : $util.dynamodb.toDynamoDBJson($context.identity.username)
    },

    "update" : {
        "expression" : "set #name = :name, imgUrl = :imgUrl, bgImgUrl = :bgImgUrl, bio = :bio, #location = :location, website = :website, birthdate = :birthdate",
        "expressionNames" : {
           "#name" : "name",
           "#location": "location",
       },

       "expressionValues" : {
            ":name" : $util.dynamodb.toDynamoDBJson($context.arguments.newProfile.name),
            ":imgUrl" : $util.dynamodb.toDynamoDBJson($context.arguments.newProfile.imgUrl),
            ":bgImgUrl" : $util.dynamodb.toDynamoDBJson($context.arguments.newProfile.bgImgUrl),
            ":bio" : $util.dynamodb.toDynamoDBJson($context.arguments.newProfile.bio),
            ":location" : $util.dynamodb.toDynamoDBJson($context.arguments.newProfile.location),
            ":website" : $util.dynamodb.toDynamoDBJson($context.arguments.newProfile.website),
            ":birthdate" : $util.dynamodb.toDynamoDBJson($context.arguments.newProfile.birthdate),
        }

    },

    "condition" : {
        "expression" : "attribute_exists(id)"
    },
}

</code></pre>
<p>The AppSync GraphQL query looks a bit like this.</p>
<pre><code class="language-javascript">const query = `mutation editMyProfile($input: ProfileInput!) {
        editMyProfile(newProfile: $input) {
          ... myProfileFields

          tweets {
            nextToken
            tweets {
                ... on Tweet {
                    ... tweetFields
                }
            }
          }
        }
    }`
</code></pre>
<p>And the ProfileInput type has this shape.</p>
<pre><code class="language-gql">input ProfileInput {
    name: String!
    imgUrl: AWSURL
    bgImgUrl: AWSURL
    bio: String
    location: String
    website: String
    birthdate: AWSDate
}
</code></pre>
<p>Within ProfileInput, only <code>name</code> is required, with the remaining values being optional.</p>
<p>In the VTL template we wrote the command as if that all the attributes exist at execution time. When they are missing DynamoDB will override those values with <code>'null'</code>, a significant issue that leads to data loss.</p>
<h2>Best practice for DynamoDB updates</h2>
<p>Updates is DynamoDB tend to require complex queries because they have a large amount of requirements, and come with the added risk of data loss.</p>
<p>Officially AWS recommends that an Update query handles these situations:</p>
<ul>
<li>handles each argument to produce a list of required updates</li>
<li>skips over any guaranteed arguments</li>
<li>if an argument is <code>null</code> or empty (<code>""</code>), that argument is removed from the item</li>
<li>if an argument is missing from the item, that argument is added</li>
<li>if an argument exists on the item, that argument is set to the new value</li>
</ul>
<p>Even though this is true for most DynamoDB update commands, the example below is an AppSync specific one.</p>
<p>These scenarios produce 3 sets of instruction lists around the directive verbs: remove, set, add.</p>
<p>The expression itself is empty and we then use the lists to populate the expression, expressionNames, and expressionValues accordingly.</p>
<p><a href="https://docs.aws.amazon.com/appsync/latest/devguide/aws-appsync-resolver-mapping-template-reference-dynamodb-updateitem.html">AWS AppSync, Mapping Template Resource / DynamoDB.UpdateItem example</a></p>
<pre><code class="language-vtl">{
    "version" : "2017-02-28",

    "operation" : "UpdateItem",

    "key" : {
        "id" : $util.dynamodb.toDynamoDBJson($ctx.args.id)
    },

    ## Set up some space to keep track of things we're updating **
    #set( $expNames  = {} )
    #set( $expValues = {} )
    #set( $expSet = {} )
    #set( $expAdd = {} )
    #set( $expRemove = [] )

    #foreach( $entry in $context.arguments.entrySet() )
        #if( $entry.key != "name" )
            #if( (!$entry.value) &#x26;&#x26; ("$!{entry.value}" == "") )
                ## If the argument is set to "null", then remove that attribute from the item in DynamoDB **

                #set( $discard = ${expRemove.add("#${entry.key}")} )
                $!{expNames.put("#${entry.key}", "$entry.key")}
            #else
                ## Otherwise set (or update) the attribute on the item in DynamoDB **

                $!{expSet.put("#${entry.key}", ":${entry.key}")}
                $!{expNames.put("#${entry.key}", "$entry.key")}

                #if( $entry.key == "ups" || $entry.key == "downs" )
                    $!{expValues.put(":${entry.key}", { "N" : $entry.value })}
                #else
                    $!{expValues.put(":${entry.key}", { "S" : "${entry.value}" })}
                #end
            #end
        #end
    #end

    ## Start building the update expression, starting with attributes we're going to SET **
    #set( $expression = "" )
    #if( !${expSet.isEmpty()} )
        #set( $expression = "SET" )
        #foreach( $entry in $expSet.entrySet() )
            #set( $expression = "${expression} ${entry.key} = ${entry.value}" )
            #if ( $foreach.hasNext )
                #set( $expression = "${expression}," )
            #end
        #end
    #end

    ## Continue building the update expression, adding attributes we're going to ADD **
    #if( !${expAdd.isEmpty()} )
        #set( $expression = "${expression} ADD" )
        #foreach( $entry in $expAdd.entrySet() )
            #set( $expression = "${expression} ${entry.key} ${entry.value}" )
            #if ( $foreach.hasNext )
                #set( $expression = "${expression}," )
            #end
        #end
    #end

    ## Continue building the update expression, adding attributes we're going to REMOVE **
    #if( !${expRemove.isEmpty()} )
        #set( $expression = "${expression} REMOVE" )

        #foreach( $entry in $expRemove )
            #set( $expression = "${expression} ${entry}" )
            #if ( $foreach.hasNext )
                #set( $expression = "${expression}," )
            #end
        #end
    #end

    ## Finally, write the update expression into the document, along with any expressionNames and expressionValues **
    "update" : {
        "expression" : "${expression}"
        #if( !${expNames.isEmpty()} )
            ,"expressionNames" : $utils.toJson($expNames)
        #end
        #if( !${expValues.isEmpty()} )
            ,"expressionValues" : $utils.toJson($expValues)
        #end
    },

    "condition" : {
        "expression"       : "version = :expectedVersion",
        "expressionValues" : {
            ":expectedVersion" : $util.dynamodb.toDynamoDBJson($ctx.args.expectedVersion)
        }
    }
}

</code></pre>
<h2>Solution</h2>
<p>For our app this means that whenever we pass a profile with an <code>undefined</code> value we will remove the existing values from the item.</p>
<p>Again we hit the data loss problem, however we can stop building the 'remove' instructions lists for undefined values.</p>
<pre><code class="language-vtl">#set( $expNames  = {} )
#set( $expValues = {} )
#set( $expSet = {} )
#set( $expRemove = [] )

#foreach( $entry in $context.arguments.newProfile.entrySet() )
    #if( $entry.key != "name" )
        #if( (!$entry.value) &#x26;&#x26; ("$!{entry.value}" == "") )
            ## If the argument is set to null, then remove that attribute from the item in DynamoDB **

            #set( $discard = ${expRemove.add("#${entry.key}")} )
            $!{expNames.put("#${entry.key}", "$entry.key")}
        #else
            ## Otherwise set (or update) the attribute on the item in DynamoDB **

            $!{expSet.put("#${entry.key}", ":${entry.key}")}
            $!{expNames.put("#${entry.key}", "$entry.key")}
            $!{expValues.put(":${entry.key}", { 
                "S": "${entry.value}"
            })}
            
        #end
    #else
        $!{expSet.put("#${entry.key}", ":${entry.key}")}
        $!{expNames.put("#${entry.key}", "$entry.key")}
        $!{expValues.put(":${entry.key}", { 
            "S": "${entry.value}"
        })}
    #end
#end

## Start building the update expression, starting with attributes we are going to SET **

#set( $expression = "" )
#if( !${expSet.isEmpty()} )
    #set( $expression = "SET" )
    #foreach( $entry in $expSet.entrySet() )
        #set( $expression = "${expression} ${entry.key} = ${entry.value}" )
        #if ( $foreach.hasNext )
            #set( $expression = "${expression}," )
        #end
    #end
#end

## Continue building the update expression, adding attributes we are going to REMOVE **
#if( !${expRemove.isEmpty()} )
    #set( $expression = "${expression} REMOVE" )

    #foreach( $entry in $expRemove )
        #set( $expression = "${expression} ${entry}" )
        #if ( $foreach.hasNext )
            #set( $expression = "${expression}," )
        #end
    #end
#end

{
    "version": "2018-05-29",
    "operation": "UpdateItem",
    "key": {
        "id" : $util.dynamodb.toDynamoDBJson($context.identity.username)
    },
    "update": {
        "expression" : "${expression}"
        #if( !${expNames.isEmpty()} )
            ,"expressionNames" : $utils.toJson($expNames)
        #end
        #if( !${expValues.isEmpty()} )
            ,"expressionValues" : $utils.toJson($expValues)
        #end
    },
    "condition" : {
        "expression" : "attribute_exists(id)"
    }
}


</code></pre>
<p>We continue to use the 'remove' lists for incoming values set to <code>null</code> or <code>""</code> eg. user wants to change their website to an empty string.</p>
<p>We no longer handle any 'add' instructions because can handle new properties with 'set'.</p>
<p>With these changes we can now support the dynamic ProfileInput object and correctly make the database changes.</p>
<h2>Testing</h2>
<p>The testing of AWS AppSync VTL templates generically involves:</p>
<ul>
<li>creating a mock context object with relevant identity and arguments (payload)</li>
<li>loading the VTL template given the mock context</li>
<li>verifying the VTL template expansion</li>
</ul>
<p>When it comes to the data verification we are looking to check:</p>
<ul>
<li>expression shape</li>
<li>expressionNames presence and values</li>
<li>expressionValues shape, presence, and values</li>
</ul>
<p>If payload newProfile contains name, bio, website</p>
<ul>
<li>Then the template expression should feature name, bio, website</li>
<li>Where all params in the expression are under the SET verb</li>
<li>The expressionNames should contain #name, #bio, #website</li>
<li>The expressionNames should have corresponding values</li>
<li>The expressionValues should contain :name, :bio, :website</li>
<li>The expressionValues should have the corresponding values</li>
<li>The expressionValues are correctly mapped as DynamoDB string ("S")</li>
</ul>
<p>If payload newProfile contains name, bio, and bio is <code>null</code> or <code>""</code></p>
<ul>
<li>Then  the template expression should feature name, bio</li>
<li>Where *name is under the SET verb</li>
<li>Where *bio is under the REMOVE verb</li>
</ul>
<h2>Notes</h2>
<ol>
<li>ExpressionValues are strings</li>
</ol>
<p>One crucial thing to note is the discrepancy between the ProfileInput types and the DynamoDB expressionValues.</p>
<p>Even though some values are of type AWSUrl or AWSDate, when given to the expressionValue they must be of type string ("S"), otherwise the query will fail.</p>
<p>Please note that this issue will trigger a false positive in testing because template itself is still value regardless of the correct value of expressionValues</p>
<ol start="2">
<li>Template evaluation with the AppSync API</li>
</ol>
<p>AppSync testing can be done by combining the "@aws-amplify/amplify-appsync-simulator" and "amplify-velocity-template" packages.</p>
<p>This method is prone to errors, and requires some output parsing of the templates after they've been loaded by the simulator eg. trimming training commas from the output.</p>
<p>A more reliable option is the <code>EvaluateMappingTemplateCommand</code> from "@aws-sdk/client-appsync". This uses an AppSync Client to evaluate any template string for validity, and it's a locally executing command.</p>
<p><a href="https://docs.aws.amazon.com/appsync/latest/APIReference/API_EvaluateMappingTemplate.html">AWS Amplify EvaluateMappingTemplateCommand documentation</a></p>
<ol start="3">
<li>VTL variations</li>
</ol>
<p>You can also try a simpler loop for building the expression.</p>
<pre><code class="language-vtl">#set($exp = "SET #name = :name")
#set($expNames = {"#name":"name"})
#set($expValues = {":name": {"S": $ctx.args.newProfile.name}})

#set($fields = ["imgUrl", "bgImgUrl", "bio", "location", "website", "birthdate"])

#foreach($field in $fields)
  #if($ctx.args.newProfile[$field] &#x26;&#x26; "$!ctx.args.newProfile[$field]" != "")
    #set($exp = "$exp, #${field} = :${field}")
    $util.qr($expNames.put("#${field}", "$field"))
    $util.qr($expValues.put(":${field}", {"S": "$ctx.args.newProfile[$field]"}))
  #end
#end

{
    "version": "2018-05-29",
    "operation": "UpdateItem",
    "key": {
        "id": $util.dynamodb.toDynamoDBJson($ctx.identity.username)
    },
    "update": {
        "expression": "$exp",
        "expressionNames": $util.toJson($expNames),
        "expressionValues": $util.toJson($expValues)
    },
    "condition": {
        "expression": "attribute_exists(id)"
    }
}

</code></pre>
<p>This is shorter, but will not handle scenarios where users require the removal of values eg. when a user wants to set the their existing profile website to <code>""</code>.</p>
<h2>Documentation</h2>
<p>Besides the AWS resources and the VTL project docs, I found the JEdit plugin writing guide to be excellent. The JEdit plugins are written in Java, but make use of VTL and the explanations of the various VTL directives are very useful and concise.</p>
<p>On testing the <code>amplify-velocity-template</code> module test suite is a great resource for ideas on how to test more complex VTL templates.</p>
<p><a href="https://docs.aws.amazon.com/appsync/latest/devguide/resolver-mapping-template-reference-dynamodb.html">AWS AppSync resolver mapping template reference for DynamoDB</a></p>
<p><a href="https://velocity.apache.org/engine/devel/vtl-reference.html#vtl-reference-contents">The Apache Velocity Project - Developer reference</a></p>
<p><a href="https://plugins.jedit.org/plugindoc/Templates/ch04.html">JEdit - Plug-in writing developer guide, VTL Directives</a></p>
<p><a href="https://github.com/aws-amplify/amplify-cli/tree/dev/packages/amplify-velocity-template/tests">Amplify Velocity Template - test suite</a></p>

                ]]>
            </description>
        </item><item>
            <title>Amplify Vite config correction</title>
            <link>https://alanionita.github.io/portfolio--svelte/blog/2025.06.11.amplify-vite-config</link>
            <pubDate>Wed, 11 Jun 2025 00:00:00 +0100</pubDate>
            <author>Alan Ionita</author>
            <description>
                <![CDATA[
                    <h1>Amplify Vite config correction</h1>
<p>Under the Vue docs for Amplify UI, we see a the following config specification.</p>
<p>https://ui.docs.amplify.aws/vue/connected-components/authenticator</p>
<pre><code class="language-javascript">// vite.config.js

export default defineConfig({
  plugins: [vue()],
  resolve: {
      alias: [
      {
        find: './runtimeConfig',
        replacement: './runtimeConfig.browser', // ensures browser compatible version of AWS JS SDK is used
      },
    ]
  }
})
</code></pre>
<p>The config is required to ensure a browser compatible version of <code>AWS JS SDK</code>.</p>
<p>Sidenotes:</p>
<ul>
<li>Why do we need the AWS JS SDK if we're using Amplify?</li>
<li>None of the docs for Amplify mention the <code>aws-sdk</code> as a dependency.</li>
</ul>
<h2>Problem</h2>
<p>Yhe logic above assumes that projects will configure Vite aliases with find / replacement pattern.</p>
<p>However the default config recommends a much simple object structure with key / value definitions.</p>
<p>For instance the Vue3 wizard produces this Vite configuration</p>
<pre><code class="language-javascript">// vite.config.js

export default defineConfig({
  plugins: [
    vue(),
    vueDevTools(),
    tailwindcss(),
  ],
  resolve: {
    alias: {
      '@': fileURLToPath(new URL('./src', import.meta.url)),
    },
  },
})
</code></pre>
<h2>Solution</h2>
<p>Although we could introduce the find / replacement pattern into the original config, it's far more sensible to replace the documentation recommendation to match the key/value format</p>
<pre><code class="language-javascript">// vite.config.js

export default defineConfig({
  plugins: [
    vue(),
    vueDevTools(),
    tailwindcss(),
  ],
  resolve: {
    alias: {
      '@': fileURLToPath(new URL('./src', import.meta.url)),
      './runtimeConfig': path.resolve(__dirname, './runtimeConfig.browser')
    },
  },
})
</code></pre>
<h2>Why does it matter?</h2>
<p>Meets users where ever they are, by matching closely the default starter config.</p>
<p>You could end up with a lot of convoluted issues with Vite because of this configuration.</p>
<p>For example a mis-configured alias would trigger errors stemming from <code>EnvironmentPluginContainer.resolveId</code> and <code>EnvironmentModuleGraph._resolveUrl</code>. This would be a false negative pointing to mis-configured environement variables as opposed to the Vite alias resolution.</p>
<p>Be careful with it!</p>

                ]]>
            </description>
        </item><item>
            <title>Amplify Documentation Safari</title>
            <link>https://alanionita.github.io/portfolio--svelte/blog/2025.06.11.amplify-docs-safari.md</link>
            <pubDate>Wed, 11 Jun 2025 00:00:00 +0100</pubDate>
            <author>Alan Ionita</author>
            <description>
                <![CDATA[
                    <h1>Amplify Documentation Safari</h1>
<p>Join me for a thrilling ride through the Amplify docs, where you have to bob and weave in order to reach your goal of connecting an AppSync backend to a Vue frontend via the Amplify service and modules.</p>
<h2>Background</h2>
<p>Amplify is an amazingly promising service for integrating frontends with GraphQL backends.</p>
<p>Common recipe for these recipes are:</p>
<ul>
<li>Backend: Cognito, AppSync with data sources as Lambda, DynamoDB, etc</li>
<li>Frontend: Vue, React etc</li>
</ul>
<p>I'd like to use Amplify for 3 purposes:</p>
<ul>
<li>Connecting to my AppSync API</li>
<li>Authenticating via Cogntio, managing tokens behind the scenes</li>
<li>Running GraphQL queries and getting data back in the UI</li>
</ul>
<h2>Resoning</h2>
<p>Amplify is an ecosystem that contains cli, scaffolding, and UI tools.</p>
<p>The combination of these tools means that we can:</p>
<ul>
<li>configure a client API to hit our AppSync backend</li>
<li>configure the authentication flow as Cognito</li>
<li>use pre-build UI components to handle the authentication stories via Cognito: sign in, sign up, login, forgot password</li>
<li>manage tokens from Cognito</li>
<li>send queries down to this API</li>
</ul>
<p>The goal of this article is to sign-post and add commentary about documentation.</p>
<p>I will however cover 2 big bonuses to using Amplify before talking about docs.</p>
<h2>Bonus - Code generation [@amplify-cli/]</h2>
<p>A big nuisance for GraphQL processes is the mirroring of queries across backend and frontend.</p>
<p>With Amplify you can with one command generate code for all Queries and Mutations from your AppSync API:</p>
<pre><code>npx @aws-amplify/cli codegen add --apiId YOUR_APPSYNC_API_ID --region YOUR_REGION

</code></pre>
<p>This generates files inside <code>graphql/</code> for <code>queries.js</code> and <code>mutations.js</code> with following examples</p>
<pre><code class="language-javascript">export const getProfile = /* GraphQL */ `
  query GetProfile($screenName: String!) {
    getProfile(screenName: $screenName) {
      id
      name
      screenName
      imgUrl
      bgImgUrl
      bio
      location
      website
      birthdate
      createdAt
      tweets {
        nextToken
        __typename
      }
      followersCount
      followingCount
      tweetsCount
      likesCount
      following
      followedBy
      __typename
    }
  }
`;

</code></pre>
<h2>Bonus - Cognito</h2>
<p>Cognito is an OAuth flow implementation and as such would require some legwork (boilerplate code) to manage keys and orchestrate the flows correctly.</p>
<p>With Amplify we get UI components, and API configurations that means we manage no tokens, vastly simplifying the app configuration.</p>
<h2>Documentation safari</h2>
<p>Let's meander through the Amplify documentation valley.</p>
<ol>
<li>Amplify Docs</li>
</ol>
<p>You would think that the best docs location is the official Amplify Docs, scoped by framework of choice. Below referencing Vue.</p>
<p>https://docs.amplify.aws/vue/</p>
<p>Most sensible place to go from here is to "Set up Amplify Data" (https://docs.amplify.aws/vue/build-a-backend/data/set-up-data/), but once on the page you realise that most the setup here involves from scratch development of both backend and frontend in parallel using Amplify.</p>
<p>The audience for Amplify seems to be folks that use it for both.</p>
<p>I'd wager a bet that most developers build AppSync with Serverless Framework, CDK, or Terraform, bypassing Amplify on the backend, whilst using Amplify libraries on the UI.</p>
<p>As someone who already has a deployed AppSync API, that means that we have to hunt and peck through the docs for the relevant code and instructions</p>
<p>What is useful here is how to create the API on the client using <code>generateClient</code> from <code>aws-amplify/data</code>. This is important because there's a huge about of different ways to do this since Amplify has a large amount of modules and techniques, each looking the same but breaking in magical ways.</p>
<ol start="2">
<li>AppSync - Integrate with your app</li>
</ol>
<p>This is actually the most sensible spot to start.</p>
<p>From the AWS Console, navigate to AppSync, then open your specific API. The instructions should appear my default on the main page.</p>
<p>One big flaw with these instructions is the use of <code>./aws-exports.js</code> which is not defined or explained.</p>
<p>We can guess that it involves configuration details, but in the following steps the configuration variables are hard-coded in the file.</p>
<p>There's no mention here about how to hide this sensitive information or how to use .env variables.</p>
<p>Also there's no configuration explanation for Cognito, even though most setups will include it.</p>
<ol start="3">
<li>Amplify UI docs</li>
</ol>
<p>Essentially the same as point 1. but more focused on the frontend. Thus more relevant for your needs. Requires framework scoping selection, below focused on Vue</p>
<p>https://ui.docs.amplify.aws/vue/connected-components/authenticator</p>
<p>The issue here is to do with the audience: even though it should be for us (someone who want to connect a UI to an existing AppSync backend via Amplify), the instructions are very simplistic.</p>
<p>The initial example doesn't cover any Sign In, Sign Up flows, nor does it explain how to render different auth components depending on the authentication states: authenticated, unauthenticated, or loading.</p>
<p>You get a lot more information within the 'Advanced Usage' section of the docs, but all of the initial instructions are replaced.</p>
<p>Arguable if these requires are 'advanced' or just base requirements.</p>
<h1>Bonus? - Sandboxing</h1>
<p>Throughout the docs sandboxing is mentioned, but never fully detailed.</p>
<p>https://docs.amplify.aws/vue/deploy-and-host/sandbox-environments/features/</p>
<p>Still murky as to how to use sandboxing with an existing API but the secrets explanation is very helpful and interesting.</p>
<p>It seems that via sandboxing you attach secrets to a sandbox and can then reference it within your backend. Again the 'backend' definition here is murky, is it a local backend generated with Amplify, is it a Vue backend, is it out initial AppSync backend?</p>
<p>This is also where they describe how to 'Generate client config' essentially producing the <code>amplify_outputs.json</code> mentioned above.</p>
<p>Unclear whether this is a bonus or not because we already have an API and there are not details on how to integrate a sandbox with a pre-deployed API.</p>

                ]]>
            </description>
        </item><item>
            <title>Strava to FOSS analysis and visualisation</title>
            <link>https://alanionita.github.io/portfolio--svelte/blog/2025.03.19.strava-to-foss</link>
            <pubDate>Wed, 19 Mar 2025 00:00:00 Z</pubDate>
            <author>Alan Ionita</author>
            <description>
                <![CDATA[
                    <h1>Strava to FOSS analysis and visualisation</h1>
<p>https://alanionita.github.io/images/posts/strava-to-foss/lead.png</p>
<p>Having used Strava for years I've always been surprised that no one else has made a better alternative.</p>
<p>The UI has been frozen in time, and most of the <em>good</em> features are pay-walled.</p>
<p>I for one refuse to pay for Strava to analyze my data. A better way to sell the subscription would be to make a service so good, so rich, so useful, that I'm begging Strava to take my money.</p>
<p>That being said, Strava is good at activity tracking. Whilst I haven't done any thorough testing, Strava has worked well for me in terms of tracking accuracy.</p>
<p>I also enjoy the interval voice notifications and the multi-sport tracking, cycling especially.</p>
<p>The challenges also, get me outside in mechanical way that I appreciate, whilst cringing that I'm encouraged to go outside by the prospect of money of very expensive kit.</p>
<p>Anyway, where do we go from here...</p>
<p>Until I roll my sleeves and warm up my LLMs to build my own tracking app, I will continue to use Strava.</p>
<p>But I'm planning to own more of my data, and interrogate it locally. And this was the inception moment for the 'Strava to FOSS' project.</p>
<h2>The Project</h2>
<p>Core features:</p>
<ul>
<li>Download activity data from Strava</li>
<li>Convert data to geospatial data</li>
<li>Use GeoPandas to explore data</li>
<li>Use OSMnx to enrich the data with OpenStreetMap nodes</li>
<li>Use OSMnx and Leaflet to generate custom maps</li>
</ul>
<p>The project is currently just a series of notebooks on Github, but overtime will be a proper service.</p>
<p><a href="https://github.com/alanionita/strava-to-foss-notebooks-python">Link to repository</a></p>
<h2>Talking to Strava</h2>
<p>The <code>strava_data.ipynb</code> notebook show an example of dealing with the Strava OAuth flow.</p>
<p>I won't go into detail, since the notebook is easy to follow. Do note that the example just gets the first 10 activities.</p>
<p>Once you reached the end of that process you will have an <code>activities.json</code>.</p>
<h2>The data</h2>
<p>For the sake of our task we need to get the route of the activity, and one way to do this is to use the <code>.map.summary_polyline</code></p>
<pre><code class="language-json">[
    {
        "map": {
       	    "id": "a13798432720",
            "summary_polyline": "",
            "resource_state": 2
	},
    }
]
</code></pre>
<p>Unclear why, but some activities don't have this entry, whilst other valid entries contain a string.</p>
<p>Also unclear is whether this value will contain elevation data, we have <code>_high</code> and <code>_low</code> elev values within the actitvity. A good mystery to uncover in a future post and feature.</p>
<blockquote>
<p>Note that most of the supporting code from this point forward is about the <code>osm.ipynb</code> notebook.</p>
</blockquote>
<h2>Geospatial primer</h2>
<h3>Geometry</h3>
<p>Quick intro to geospatial data is that most things relate to Lines, Polylines, and Polygons.</p>
<p>We normally work with these basic geometric shapes using a low level lib called Shapely.</p>
<p>Whenever we use a geospatial framework, like GeoPanda, they in turn will use Shapely under the hood.</p>
<p>Normally you just get the data in a nice format, but when you're unlocking existing data you will need to implement Shapely for data conversions.</p>
<h3>Coordinates</h3>
<p>Another important concept with geospatial data is the use of coordinates: latitude, longitude.</p>
<p>You'll see below that in order to make our lives easier we convert original PolyLine data to a geospatial list of coordinates.</p>
<p>I can't stress to you how useful this can be for future analysis.</p>
<p>Take a regular 20km run as geospatial lists: we end up with ~300 coordinate points, we can then compare distances between these, track time across each coordinate point and more.</p>
<h3>Formats and GeoInfra</h3>
<p>Geospacial data use a JSON flavour called <code>.geojson</code>. You'll see below that there's a big fuss about saving our converted data to <code>.geojson</code> so that we do the format conversion in one step and end up with a portable reusable geospatial file for later geospatial focused tasks.</p>
<p>We can go further and instead of saving to a file, we can save the data to a database like PostGIS. That's not covered here, but stay tuned.</p>
<h2>Geospatial data</h2>
<p>Back to our Strava data.</p>
<ol>
<li>Get the coordinate points</li>
</ol>
<p>This <code>summary_polyline</code> string is a Google Maps encoded Polyline, a proprietary format.</p>
<p>We will need to decode this using Shapely, because we want the coordinate points.</p>
<ol start="2">
<li>Convert them to the right coordinate reference system</li>
</ol>
<p>We need to understand what cartographic system did they originally use in their encoding. I'll spare you the head scratching, Google Maps uses Mercator system or EPSG:3857.</p>
<p>Best way to imagine the CRS is to imagine yourself in space, floating about. Now imagine that your friend is also there with you, floating around like a maniac. Great! Now I ask that both you and your friend look down at the Eifel tower, manifique!</p>
<p>You and your friend will be looking at the same coordinate (lat,lng), but you each have a different angle to that coordinate. As such you each have your or CRS.</p>
<p>We do this crs conversion via GeoPandas.</p>
<ol start="3">
<li>Working with GeoPandas</li>
</ol>
<p>When we use GeoPandas we need to also deal with how it expects coordinates. By default GeoPandas expects a crs called WGS84 or EPSG:4326.</p>
<p>When we load the data from the original Polyline conversion we need to load it as WGS84 and then change the crs to Mercator.</p>
<p>But we're not done quite yet. You see that WGS84 crs is very important for geographic operations inside GeoPandas. Mapping, distance, comparisons are all done in the WGS84 crs.</p>
<p>Before we start doing these geographic operations we need to make sure that we convert back from Mercator to WGS84.</p>
<p>What happens if we load the data as WSG84?</p>
<p>Try it and see (once you set up the mapping of the data).</p>
<p>If you want to take my word for it, the positions are inaccurate. If you have a clear start and end point they will be shifted by a few meters.</p>
<ol start="4">
<li>Correct format for lat,lng</li>
</ol>
<p>Each mapping technology will have their own system, Folium for example requires (lat, lng). The normal format.</p>
<p>Others require the 'goofy' format (lng, lat), looking at you GeoPandas and Google Maps. There may be a valid reason, but that is smaller that my ignorance atm.</p>
<p>For the record when geospatial data is inside GeoPandas it's usually (lng, lat), so remember to do a switch before you try to map the data.</p>
<p>What happens if you don't?</p>
<p>Northwest England coordinates, get mapped in the Bay of Mogadishu, Somalia. I guess you'd call it the other side of the world somehow.</p>
<ol start="5">
<li>Polygons</li>
</ol>
<p>This is where we leverage some of these coordinates to draw a shape around them.</p>
<p>Using these Polygons we can then make filters for calls to other geospatial services like OpenStreetMaps</p>
<h2>OpenStreetMaps</h2>
<p>Still very much a black hole for me, but at the core we want to map our coordinates line to an OpenStreetMap map.</p>
<p>We could be naive and just get the whole map, but that's a huge amount of data.</p>
<p>Instead we use our coordinates to create a polygon, and we filter the OSM data with that polygon. This is where depending on the situation you might have to add borders to your polygon.</p>
<p>Worth noting that this step is incredible fiddly because all OSMnx requests are made via a limited open-source api. Even if it weren't limit, hammering the api would be akin to kicking puppies. Don't do it!</p>
<p>The adjustment of the polygon thus makes sure that we only query what is necessary.</p>
<p>What we get from these queries is the <code>graph</code>.</p>
<p>In order to tell OSM what to put in the graph we can specify the network types: road, walk, cycling etc. As well as other filters.</p>
<h2>Mapping with Folium</h2>
<p>Just a remind we're aiming to map the Strava activity onto an OpenStreetMap graph of nodes.</p>
<p>Folium will:</p>
<ul>
<li>take the route DataFrame, and the OSM graph</li>
<li>use the route to define the center of the map</li>
<li>use the graph to define the edges of the map</li>
<li>then plot the route as a <code>folium.PolyLine()</code></li>
<li>then plot the route start and end as <code>folium.Marker()</code></li>
</ul>
<h2>Pulling it together</h2>
<p>In conclusion, to free our Strava data we need to:</p>
<ul>
<li>Read the Google Maps polyline data and convert to (lat,lng)</li>
<li>Remap that coordinate line to the correct CRS and regular or goofy (lat, lng) pairs</li>
<li>Create a polygon from the coordinates</li>
<li>Use the polygon to extract data from OpenStreetMap into a graph</li>
<li>Generate a map from the coordinates and the graph</li>
</ul>
<p>As a final touch we output the map to an html file, which you can see <a href="https://alanionita.github.io/strava-to-foss-notebooks-python/">here</a></p>
<h2>Conclusion</h2>
<p>This is just scratching the surface of what is possible, and I strongly recommend you give it a try.</p>
<p>For the next iteration, I'll be fetching all my Strava data, converting it to geospatial, and save it inside a database.</p>
<p>Final result</p>
<p>https://alanionita.github.io/images/posts/strava-to-foss/output.png</p>

                ]]>
            </description>
        </item><item>
            <title>One Big Text File (obtf)</title>
            <link>https://alanionita.github.io/portfolio--svelte/blog/2025.03.03.obtf</link>
            <pubDate>Mon, 03 Mar 2025 00:00:00 Z</pubDate>
            <author>Alan Ionita</author>
            <description>
                <![CDATA[
                    <h1>One Big Text File (obtf)</h1>
<p>Learning from <a href="https://ellanew.com/tagged/obtf">Ellane W</a>, I started journaling inside a "one big text file".</p>
<h2>Context</h2>
<p>My note-taking system is text based, predominantly Markdown files, structure using the PARA system:</p>
<ul>
<li>Projects  -> <code>./project.*.md</code></li>
<li>Area      -> <code>./area.*.md</code></li>
<li>Reference -> <code>./reference.*.md</code></li>
<li>Archive   -> <code>./arch.*.md</code></li>
<li>Journal   -> <code>./journal.*.md</code></li>
</ul>
<p>Project - Most large chunks of work go into a project, if it's not big enough for a project, it usually goes in the other bins. Recently I added yearly sub-scoping <code>project.2025.*.md</code>. eg project.2025.portfolio-svelte.md, project.2025.kafka-pipeline.md, etc. This helps with yearly reviews, when I can easily <code>ls project.2025*</code> to get all the in progress / complete projects in a year.</p>
<p>Archive - When I no longer need a note, it goes into archives. Over time the aim is to summarise the archives and discard the notes. Holding onto notes seems redundant. They should be distilled into "thoughts" or "timelines" for each topic, reflecting your current take on each topic.</p>
<p>Area - Meant to keep track of various parts of life. eg area.personal.md, area.work.md, area.sport.md etc. Vastly under-used at the minute within my system.</p>
<p>References - Used to store links and useful ideas on specific topics. In my case it's full of reference scoped by tech eg. reference.docker.md, reference.sql.md, reference.nextjs.md. Again vastly under-used at the minute.</p>
<p>Journal - Stores daily, quarterly, yearly notes, and this is where most of the note-taking happens. The ideal workflow is: jot down notes within a <code>journal.*.md</code> note and at the end of day, review, summarise, and update <code>area.*.md</code> or <code>resource.*.md</code>. In reality it's just a dumping ground.</p>
<p>All the notes are stored within a cloud-hosted git repo.</p>
<h2>The problem</h2>
<p>In 2025 I wrote ~200 notes, most of which are scoped under <code>journal.daily.*.md</code> leaves. My lax approach to reviewing the notes, means that whenever I'm ready to review the journal entries, I end up with dozens of files to peruse and summarise into a weekly / quarterly superset.</p>
<p>Moving files into <code>archive.*</code> is not easy unless you automate it with the script below.</p>
<h2>Solution</h2>
<p>Instead of ~200 leaves of <code>journal.daily.*.md</code> files have one yearly-scoped journal file <code>journal.2025.md</code></p>
<p>Within the file in order to add some keywords and ease search retrieval I implemented a key - very heavily inspired by Ellane W. suggestions.</p>
<pre><code class="language-markdown">T. Task
X. Task done
N. Note
P. Project
R. Reference
</code></pre>
<p>The entries in the file are all collected under a day heading <code># 03-03-2025, Monday</code>, and new calendar entries are placed at the top of the file.</p>
<h2>Observations</h2>
<p>The "obtf" is handy for reviews already, making it dead-easy to see past days note with a simple scroll vs opening various files.</p>
<p>Search and retrieval is great with <code>grep</code> too, and using one big text file means that I use less <code>fzf</code> scripts or <code>grep -nr "term" .</code>.</p>
<p>The new journal opens in vim via terminal alias <code>,journal</code>, and this allows me wicked fast scribbles.</p>

                ]]>
            </description>
        </item><item>
            <title>FOSS tools complex?</title>
            <link>https://alanionita.github.io/portfolio--svelte/blog/2025.02.21.foss-tools-complex</link>
            <pubDate>Fri, 21 Feb 2025 00:00:00 Z</pubDate>
            <author>Alan Ionita</author>
            <description>
                <![CDATA[
                    <h1>FOSS tools complex in 2025?</h1>
<p>Apache Kafka for example has an air of complexity and flamboyance around it.</p>
<p>A vast majority of Hacker News comments rag on Kafka, the latest comments making it sound like a "CV-padding technology, needlesly complex, overtly unnecessary".</p>
<p>Similar myths around Elastic Search: complex, hard to maintain, expensive.</p>
<p>If I were too include languages here, Haskell would be the top contender: considered too academic, impractical, obtuse, and a full stop questionable choice.</p>
<p>What is the real damage of these myths?</p>
<p>Because of the grapevine new people don't use Kafka, new people avoid Elastic Search.</p>
<p>This perpetraits a scenario where the hard tools remain hard because new people can't shape their structure and usage. Without new users the tooling teams can't understand the new UX problems they need to solve.</p>
<p>More importantly where are these people to going for replacements?</p>
<p>For Kafka replacements, people go to the cloud. They end up using AWS Kinesis streams, or AWS EventBridge - both expensive and complex services. Argueably as complex as Kafka and closed source.</p>
<p>For Elastic Search replacements, people go to the cloud again, to database provider services like MongoDB Atlas, or to AWS OpenSearch. Both of these options are costly and just as complex.</p>
<p>Cost in itself should be a criteria for businesses, if a managed service costs but is more easy to manage then we need to asses the trade-off. Sometimes however the costs act as feature gates and the managed services are just as hard to maintain or grok.</p>
<p>For individuals, cost is a barier, no doubt about it. Yet, despite this barrier, the industry wants developers who can build real-time systems or complex text-search.</p>
<p>Where and how do we get these new developers? Where and how do they gain this experience?</p>
<p>Cloud services, costs, and cost-gating features limits the market from gaining new developers, and limits developers from learning these tools.</p>
<h2>Kafka and the Apache stack</h2>
<p>Apache Kafka for the most part is FOSS. It has for-cost closed source components and services, but the basic streaming is there. Kafka can also run in a homelab and is not resource hungry.</p>
<p>For the next few days I plan to post further updates on this: Kafka for a timeseries data injestion pipeline.</p>
<p>In other words a "soft real-time" problem.</p>
<h2>Elastic Search</h2>
<p>In 2020 I used a cloud managed Elastic Search service through AWS. This was before their business bust up.</p>
<p>At the time, it wasn't considered a good architecture option internally, because of costs. Maintenance was also a factor.</p>
<p>Our challenge was to build real-time VOIP telecom systems, transcribe the call, and make it available within a UI. This is a "hard real-time problem", since all data goals must be met.</p>
<p>Elastic Search allowed us to achieve transcription, indexing, and retrieval, in less than 200ms. With networking done over WebSockets, in a event-driven system.</p>
<p>Despite the results, Elastic Search wasn't recommended because of costs and maintenance.</p>
<p>On management, our index required used 3 shards, and a fairly high spec set of machines. No where near as complex as it could get; a baby Elastic Search setup.</p>
<p>Fast forward to 2025 and using Elastic Search a vastly improved experience, having managed to run a node with 1G memory, bulk index in &#x3C; 30s, and make text search calls with &#x3C;200ms retrival. Not the largest index in the world, but I can tell that even with 1G limitation it can take more documents and I don't expect the retrieval to be impacted.</p>
<p>Elastic Search is FOSS, configuration was straighforward, and the query experience is modern and straightforward - GraphQL-like in a way.</p>
<h2>Summary</h2>
<p>Without Elastic Search, how else can you build text search systems at home on a Raspberry Pi?!</p>
<p>Without Kafka how else can you ingest timeseries data from MCUs or replicate AWS Kinesis and AWS EventBridge services?</p>
<p>Both are important tools for modern software development, and should be on everyone's radar who wants design data-intensive apps.</p>
<p>Most importantly, they are FOSS and local, and grant you full control.</p>
<p>Just add a pinch of energy and time investment.</p>

                ]]>
            </description>
        </item><item>
            <title>SveleteKit - reactivity quirk</title>
            <link>https://alanionita.github.io/portfolio--svelte/blog/2025.02.14.sveltekit-reactivity</link>
            <pubDate>Fri, 14 Feb 2025 00:00:00 Z</pubDate>
            <author>Alan Ionita</author>
            <description>
                <![CDATA[
                    <h1>SveleteKit - reactivity quirk</h1>
<p>Svelte is great for heavy web scenarios: maps, visualizations, animations etc. And it's common that most of these scenarios depend on Web API.</p>
<p>Most of the times we use the <code>$app/environment</code> browser designation to allow for server process to fully complete, and then execute the Web API components, as gate by <code>browser</code> designation.</p>
<p>eg. Map example</p>
<pre><code class="language-javascript">&#x3C;script lang="ts">
	import { browser } from '$app/environment';
	import Map from '$lib/components/map.svelte';
&#x3C;/script>


{#if !browser}
	&#x3C;h2>Loading map...&#x3C;/h2>
{:else}
	&#x3C;Map bind:lat bind:lng />
{/if}
</code></pre>
<p>In Svelte this behavior works correctly to bypass any server process required. Ultimately this is a premature optimization in Svelte because it is largely browser-dependent.</p>
<p>From here on you may notice that Svelte and SvelteKit are used interchangeably. Svelte mentions above are the only place where they can ONLY mean the Svelte frontend library.</p>
<h2>Sveltekit</h2>
<p>To expand on the Map example, it's common that Map libraries include <code>IntersectionObservers</code>, or <code>ResizeObservers</code>, or other Web APIs required for complex interaction.</p>
<p>The above example of browser gating will fail when implemented within a SvelteKit application, usually with the error that <code>X API cannot be found</code>, causing confusion.</p>
<p>Why is a browser gated piece of code still running on the server? Who is running? These are questions I've yet to answer.</p>
<p>However there is a way to avoid this behavior and bypass the errors.</p>
<h2>Svelte / Page options</h2>
<p>Official documentation on page options -> https://svelte.dev/docs/kit/page-options</p>
<blockquote>
<p>You can control each of these on a page-by-page basis by exporting options from +page.js or +page.server.js, or for groups of pages using a shared +layout.js or +layout.server.js.</p>
</blockquote>
<p>Within that +layout.ts file we need to focus on 2 page options specifically:</p>
<ul>
<li>ssr</li>
<li>csr</li>
</ul>
<ul>
<li></li>
</ul>
<h2>Svelte / Page options / ssr</h2>
<blockquote>
<p>Normally, SvelteKit renders your page on the server first and sends that HTML to the client where it’s hydrated. If you set ssr to false, it renders an empty ‘shell’ page instead. This is useful if your page is unable to be rendered on the server (because you use browser-only globals like document for example), but in most situations it’s not recommended (see appendix).</p>
</blockquote>
<p>Turning this option off does indeed fix the problem of browser-dependencies being run outside of a browser gate.</p>
<pre><code class="language-javascript">// +layout.ts
export const ssr = false;

</code></pre>
<p>But the following notice in the docs sounds concerning and mysterious.</p>
<blockquote>
<p>Even with ssr set to false, code that relies on browser APIs should be imported in your +page.svelte or +layout.svelte file instead. This is because page options can be overridden and need to be evaluated by importing your +page.js or +layout.js file on the server (if you have a runtime) or at build time (in case of prerendering).</p>
</blockquote>
<p>What does importing mean here? In the majority of cases where this becomes a problem, we experience the problem when importing from an external module, which depends on Web APIs within.</p>
<p>Map example again: lib/components/map.svelte</p>
<pre><code class="language-javascript">// Map.svelte
&#x3C;script lang="ts">
	import { onMount, onDestroy } from 'svelte';
	import { browser } from '$app/environment';
	import WebMap from '@arcgis/core/WebMap';
	import MapView from '@arcgis/core/views/MapView';

	let mapView: MapView | undefined = $state();
	let mapContainer: HTMLDivElement | undefined = $state();
    	let { lng = $bindable(), lat = $bindable() } = $props();

	onMount(() => {
		// Initialize the map only in the browser
		mapView = new MapView({
			container: mapContainer
			center: [lng, lat]
		})
		
	});

	onDestroy(() => {
		if (mapView) {
			mapView.destroy();
		}
	});
&#x3C;/script>

{#if !browser}
    &#x3C;p>Loading map...&#x3C;/p>
{:else}
    &#x3C;div bind:this={mapContainer} class="arcgic-map">&#x3C;/div>
{/if}

</code></pre>
<p>This example will trigger a <code>ResizeObserver</code> error, even though we have gated the code within the browser.</p>
<p>The errors is triggered within an ARCGIS SDK library.</p>
<p>Again what does importing mean here? We have technically imported the code and gated it correctly. And since the Map component is imported by the <code>+page.svelte</code> that should pass the criteria outlined.</p>
<p>What else is needed?</p>
<p>Do we have to import the offending API itself? Surely not, since that would also require up to shadow the existing Web API (when available) with an manually imported one.</p>
<h2>Svelte / Page options / csr</h2>
<blockquote>
<p>Ordinarily, SvelteKit hydrates your server-rendered HTML into an interactive client-side-rendered (CSR) page. Some pages don’t require JavaScript at all — many blog posts and ‘about’ pages fall into this category. In these cases you can disable CSR:</p>
</blockquote>
<p>This setting is not technically a requirement, but is a bit of configuration that translates what we want to expect: we want the Map page to always run Javascript because it contains an external module that has it's own features and requirements.</p>
<pre><code class="language-javascript">// +layout.ts
export const ssr = false;
export const csr = true; 
</code></pre>
<h2>Svelte / Page options / prerender</h2>
<p>This is a counter intuitive naming. Although it does configure prerendering, the purpose for using it is nuanced.</p>
<p>In our case we can't prerender a Map page, since they usually involved regular requests for: loading, zooming, fetching tiles etc. All managed by external libraries.</p>
<p>But we do want to prerender the page itself.</p>
<p>Map pages are usually a code injection scenario: we inject our dynamic element into a static HTML. This is a one-way binding, in other words we inject the map and only interact with it there on.</p>
<p>Sometimes we may have a dynamic element that is external to the map and is tied to it in some way: a filter that changes as the map changes, a banner that updates as we move the map, a sector select that moves the map as we change sectors. All of which are still code injection scenarios, with 2-way bind.</p>
<p>Both scenarios are compatible with prerendering: since we only want to route to be prerendered, and the html by proxy.</p>
<p>Routes with prerender true are excluded from manifests, so our server is tiny.</p>
<p>In my use case I want dynamic Maps within static files, no servers involved.</p>
<p>All routes are default <code>prerendered = true</code> and the Svelte <code>adapter-static</code> is used.</p>
<pre><code class="language-javascript">// +layout.ts
export const ssr = false;
export const csr = true;
export const prerender = true;
</code></pre>
<p>Of note prerender can also be 'auto', where you want popular routes to be pre-rendered and historic content to be server-rendered.</p>
<p>Unclear at what scale this is required, because a server rendering content is probably more maintenance and resource-heavy than fully static build.</p>
<p>The fully static build takes longer to compile, but once deployed it's essentially free. With CI/CD and strong hardware this long compile time is a minor negative point.</p>
<p>Either way, the "auto" option is there.</p>
<p>For larger prerendered builds it's important to also consider <code>entries</code>. These need to pre-build to ensure the best prerendering performance. I will cover them in a future post.</p>
<h2>Summary</h2>
<p>Strange behavior and even stranger docs, however the configuration above does produce the expected results.</p>
<p>In hindsight I'd expect the handling of CSR and SSR to be more consistent, but with most frameworks preferring 'server' options it's to be expected that CSR behavior is not quite there by default.</p>

                ]]>
            </description>
        </item><item>
            <title>Svelte - v5 Migration</title>
            <link>https://alanionita.github.io/portfolio--svelte/blog/2025.02.13.svelte-5-migration</link>
            <pubDate>Thu, 13 Feb 2025 00:00:00 Z</pubDate>
            <author>Alan Ionita</author>
            <description>
                <![CDATA[
                    <h1>Svelte - v5 Migration</h1>
<p>Big fan of SvelteJS as framework and in particular the Svelte monolith that is SvelteKit.</p>
<p>I maintain a scaffolding repo which act as a base to most Svelte projects I create. You can see that scaffold <a href="https://github.com/alanionita/scaffold-sveltekit-tailwind-shadcn">here</a>.</p>
<p>As of October 2024 Svelte released Svelte 5, their biggest release yet as covered <a href="https://svelte.dev/blog/svelte-5-is-alive">here</a>, but it brings some significant changes to day to day Svelte writing and patterns.</p>
<p>This article aims to show:</p>
<ul>
<li>How to easily upgrade a SvelteKit 2.0 project using Svelte 4</li>
<li>What the changes are and why they've come about</li>
</ul>
<h2>Upgrading</h2>
<p>Upgrading in Svelte couldn't be easier, because of the <a href="https://svelte.dev/docs/cli/sv-migrate">sv migrate tooling</a>.</p>
<h3>Usage</h3>
<p>Within the repo run <code>npx sv migrate TARGET</code> where target here is svelte-5, so <code>npx sv migrate svelte-5</code>. Check the sv migrate docs for other available targets.</p>
<p>sv migrate will then ask:</p>
<ul>
<li>Ask about current dir, since some of the change will impact a mono-repo</li>
<li>Ask which folders to refactor</li>
<li>Ask if you want to tooling to convert components to new syntax</li>
</ul>
<p>My repo is fairly simple so I went ahead and said yes and selected all the option on the above.</p>
<h2>Changes - Packages</h2>
<p>One thing of note here about <code>sv migrate</code> vs a manual package upgrade is that you don't need to trial a bunch of module versions before you reach the right mix.</p>
<p>Having tried it before I will tell you now that: upgrading all packages to latest will break your build.</p>
<p>sv migrate already contains the right module combinations and here are the changes.</p>
<pre><code>Updated svelte to ^5.0.0
Updated @sveltejs/kit to ^2.5.27
Updated @sveltejs/vite-plugin-svelte to ^4.0.0
Updated prettier-plugin-svelte to ^3.2.6
Updated eslint-plugin-svelte to ^2.45.1
Updated typescript to ^5.5.0
Updated vite to ^5.4.4
</code></pre>
<p>Latest versions for reference</p>
<pre><code>svelte                      5.20.0
@sveltejs/kit               2.17.1
@sveltejs/vite-plugin-svelte 5.0.3
prettier-plugin-svelte       3.3.3
eslint-plugin-svelte        2.46.1
typescript                   5.7.3
vite 	                     6.1.0
</code></pre>
<p>As you can see things move fast, and in the 3 months since release we've already had a fair bit of drift. This is mostly fine since Svelte is relatively stable.</p>
<p>Of most concerns is the vite major release of v6, and the large number of minor versions on the svelte packages.</p>
<h3>Vite v5 vs v6</h3>
<p>In my context I'm ok with a lagging codebase and will upgrade whenever crutial.</p>
<p>For anyone else who might be impacted I wanted to summarise what changed in vite v6, Read further in the migration guide, <a href="https://vite.dev/guide/migration.html">https://vite.dev/guide/migration.html</a>.</p>
<p>v6 updates:</p>
<ul>
<li>Environement API refactors; experimental api included which changes to interface; opt-in only</li>
<li>Runtime API: experimental Module Runner API, requires using the new API after updating</li>
<li>resolve.conditions: impacts those with .conditions config; from v6 conditions need to be included manually as oposed to being defaulted, allows ssr and regular configs to be different (whilst before ssr would inherit from regular config), if you have previous config in v6 you need to add the defaults per resolution</li>
<li>JSON.stringify: in v6 json.strigify: true no longer changes json.namedExports: disabled; json.namedExports is respected; strigify also has a new value of auto, which only stringifies large objects.</li>
<li>Larger support for asset references in HTML element: v5 only  and  could reference assets. v6 has an extended list <a href="https://vite.dev/guide/features#html">https://vite.dev/guide/features#html</a></li>
<li>CSS: postcss-load-config updated in v6, sass uses modern api by default, custom CSS output file in library mode</li>
</ul>
<h3>Svelte minor releases difference</h3>
<p>Because of the volume of releases there's a possibility here that we might be missing key bug fixes.</p>
<h3>Installing packages</h3>
<p>Now lets install the migrated packages and see what happens.</p>
<ol>
<li>Remove previous installed packages</li>
</ol>
<p><code>rm -rf node_modules/ &#x26;&#x26; rm package-lock.json</code></p>
<ol start="2">
<li>Install with package of choice (npm)</li>
</ol>
<p><code>npm install</code></p>
<p>Issues:</p>
<ul>
<li>
<ol>
<li><code>@sveltejs/vite-plugin-svelte@^4.0.0</code> not found (see logs below)</li>
</ol>
</li>
<li>
<ol start="2">
<li></li>
</ol>
</li>
<li>
<ol start="3">
<li>{@render children?.()} throwing error about unrecognised @ symbol</li>
</ol>
</li>
</ul>
<pre><code>npm error code ERESOLVE
npm error ERESOLVE could not resolve
npm error
npm error While resolving: streak-tracker--svelte@0.0.1
npm error Found: @sveltejs/vite-plugin-svelte@3.1.2
npm error node_modules/@sveltejs/vite-plugin-svelte
npm error   dev @sveltejs/vite-plugin-svelte@"^4.0.0" from the root project
npm error   peer @sveltejs/vite-plugin-svelte@"^3.0.0 || ^4.0.0-next.1" from @sveltejs/kit@2.6.4
npm error   node_modules/@sveltejs/kit
npm error     dev @sveltejs/kit@"^2.5.27" from the root project
npm error     peer @sveltejs/kit@"^2.0.0" from @sveltejs/adapter-auto@3.2.5
npm error     node_modules/@sveltejs/adapter-auto
npm error       dev @sveltejs/adapter-auto@"^3.0.0" from the root project
npm error   1 more (@sveltejs/vite-plugin-svelte-inspector)
npm error
npm error Could not resolve dependency:
npm error dev @sveltejs/vite-plugin-svelte@"^4.0.0" from the root project
npm error
npm error Conflicting peer dependency: svelte@5.20.0
npm error node_modules/svelte
npm error   peer svelte@"^5.0.0-next.96 || ^5.0.0" from @sveltejs/vite-plugin-svelte@4.0.4
npm error   node_modules/@sveltejs/vite-plugin-svelte
npm error     dev @sveltejs/vite-plugin-svelte@"^4.0.0" from the root project
npm error
npm error Fix the upstream dependency conflict, or retry
npm error this command with --force or --legacy-peer-deps
npm error to accept an incorrect (and potentially broken) dependency resolution.
npm error
npm error
npm error For a full report see:

</code></pre>
<h3>Fixing errors</h3>
<ul>
<li>
<ol>
<li>@sveltejs/vite-plugin-svelte: not a real error, but I suspect an issue with timeout during the npm i process; making sure I have a clean install by removing node_modules and package-lock.json allowed me to get a successful install without the error</li>
</ol>
</li>
<li>
<ol start="2">
<li>script tag issue with "@sveltejs/adapter-auto": fixed it by updating the version to ^4.0.0</li>
</ol>
</li>
<li>
<ol start="3">
<li>noticed an issue with tsconfig.json, whereby "moduleResolution" required "module" to be present and set to preserve; after adding that value the IDE refreshed and the @render issue went away</li>
</ol>
</li>
</ul>
<h2>Changes - Components</h2>
<h3>Runes</h3>
<p>Multiple changes stem from the new Runes API: compiler instructions that tell Svelte about reactivity. Runes start with <code>$</code></p>
<h3>let -> $props</h3>
<p>In v4 props were achieve using <code>export let xyz = "xyz"</code> declarations. These declarations made content available from  into the main body of the component.</p>
<p>In v5 all props come from the $props rune as seen below:</p>
<pre><code class="language-javascript">&#x3C;script>
	let { optional = 'unset', required } = $props();
&#x3C;/script>
</code></pre>
<p>In my case, the app is also using Typescript so there are further changes on the type definitions, which will be covered later.</p>
<p>v5</p>
<pre><code class="language-javascript">&#x3C;script lang="ts">
	import type { HTMLAttributes } from "svelte/elements";
	import { cn } from "$lib/shadcn.js";

	type $$Props = HTMLAttributes&#x3C;HTMLDivElement>;

	let className: $$Props["class"] = undefined;
	export { className as class };
&#x3C;/script>

&#x3C;div
	class={cn("bg-card text-card-foreground rounded-lg border shadow-sm", className)}
	{...$$restProps}
>
	&#x3C;slot />
&#x3C;/div>

</code></pre>
<p>v6</p>
<pre><code class="language-javascript">&#x3C;script lang="ts">
	import type { HTMLAttributes } from "svelte/elements";
	import { cn } from "$lib/shadcn.js";

	type $$Props = HTMLAttributes&#x3C;HTMLDivElement>;

	interface Props {
		class?: $$Props["class"];
		children?: import('svelte').Snippet;
		[key: string]: any
	}

	let { class: className = undefined, children, ...rest }: Props = $props();
	
&#x3C;/script>

&#x3C;div
	class={cn("bg-card text-card-foreground rounded-lg border shadow-sm", className)}
	{...rest}
>
	{@render children?.()}
&#x3C;/div>
</code></pre>
<h3>render method no longer compiled for SSR</h3>
<p>This is specifically related to SSR application, but worth mentioning.</p>
<h3>children() and snippets</h3>
<p>In v4 the main paradigm for creating higher order components was to use .</p>
<p>In v5 they are deprecated in favour of  constructs which increase the flexibility. Also added in v6 a clearer construct for children.</p>
<p>In my case I've only used  for HOCs and those instances have been refactored to use children instead</p>
<p>v4</p>
<pre><code class="language-javascript">&#x3C;script>
	import '../app.css';
&#x3C;/script>

&#x3C;slot>&#x3C;/slot>
</code></pre>
<p>v5</p>
<pre><code class="language-javascript">&#x3C;script lang="ts">
	import '../app.css';
	interface Props {
		children?: import('svelte').Snippet;
	}

	let { children }: Props = $props();
&#x3C;/script>

{@render children?.()}
</code></pre>
<h3>Component typing changes</h3>
<p>Most of the codebase saw change related to the new children component which is typed as Snippet from 'svelte' package.</p>
<p>No other significant changes present, but be advised that there are further changes detailed in the release note <a href="https://svelte.dev/docs/svelte/v5-migration-guide#Components-are-no-longer-classes-Component-typing-changes">https://svelte.dev/docs/svelte/v5-migration-guide#Components-are-no-longer-classes-Component-typing-changes</a></p>
<h3>let -> $state</h3>
<p>Covered here last because I haven't implemented any states within this template.</p>
<p>I can see this being an important change to keep in mind in production apps since the let definition is common.</p>
<p>This essentially brings the biggest headaches, since prior to v5 developers built up this pattern of separation between prop definitions and state. This separation is still present in v5 but the change of let -> $props will confuse readers who will see more let -> $state.</p>
<p>I actually agree with the pattern separation, but getting used to it will take some time, mostly because the old definitions were worse.</p>
<p>v4</p>
<pre><code class="language-javascript">&#x3C;script>
	let count = 0;
&#x3C;/script>

</code></pre>
<p>v5</p>
<pre><code class="language-javascript">&#x3C;script>
	let count = $state(0);
&#x3C;/script>
</code></pre>
<h2>Summary</h2>
<p>Big update, one of the biggest yet! Largely positive, but with significant gotchas and implementation details.</p>
<p>Overall a move in the right direction, but shifting into the new paradigms will cause some headscratching.</p>

                ]]>
            </description>
        </item><item>
            <title>Flutter - Development Setup</title>
            <link>https://alanionita.github.io/portfolio--svelte/blog/2025.02.12.flutter-setup</link>
            <pubDate>Wed, 12 Feb 2025 00:00:00 Z</pubDate>
            <author>Alan Ionita</author>
            <description>
                <![CDATA[
                    <h1>Flutter - Development Setup</h1>
<p>Initial dive into Flutter for Desktop Linux development.</p>
<p>Starting from scratch, with the below guide, and although I have read about Flutter previously I've not build anything until now. I do have some experience with Android development, but I will actively try to avoid using that experience, probably forgot it anyway.</p>
<p>Going through the install guide found <a href="https://docs.flutter.dev/get-started/install/linux/desktop">here</a></p>
<h2>Flutter install</h2>
<h3>Hardware requirements</h3>
<p>Support for Linux is there and automatically detected by the site. Nice landing page too, but frankly I just skipped to installation - don't need to read the marketing, since I already want to use it.</p>
<p>Hardware requirements are high. On Linux especially, we need to expect lesser performance since we expect folks to use older software.</p>
<p>As an ethos too, Linux enables more usage out of older software so the requirements will end us (the developers) push for hardware updates. To me, hardware updates are fine for commercial software, but for personal software it's just enough friction for someone to stop using your app.</p>
<p>| Requirement   | Minimum   | Recommended     |
| ------------- | --------- | --------------- |
| CPU cores     | 4         | 8               |
| Memory        | 8         | 16              |
| Display res   | 1366x768  | 1920x1080       |
| HDD           | 4GB       | 52GB            |</p>
<p>If you were a retro enthusiast and were working on a Core 2 Duo device you're out of luck. And to add to that, 16GB RAM recommended.</p>
<h3>Software requirements</h3>
<p>All in all 12 environment level dependencies for developing Linux apps.</p>
<p>As always I created a Flutter Ansible playbook to make it easier to quickly setup a new machine.</p>
<pre><code class="language-yaml">- name: Flutter Development Setup
  hosts: my_local
  vars:
    # Useful in case you want to uninstall the whole playbook later
    #    NB: doublecheck that you don't uninstall curl, git - stuff you need
    state: present # or 'absent'
  tasks:
    - name: curl - install
      apt:
        name: curl
        state: '{{state}}'
    - name: git - install
      apt:
        name: git
        state: '{{state}}'
    - name: unzip - install
      apt:
        name: unzip
        state: '{{state}}'
    - name: xz-utils - install
      apt:
        name: xz-utils
        state: '{{state}}'
    - name: zip - install
      apt:
        name: zip
        state: '{{state}}'
    - name: libglu1-mesa - install
      apt:
        name: libglu1-mesa
        state: '{{state}}'
    - name: clang - install
      become: true
      apt:
        name: clang
        state: '{{state}}'
    - name: cmake - install
      become: true
      apt:
        name: cmake
        state: '{{state}}'
    - name: ninja-build - install
      become: true
      apt:
        name: ninja-build
        state: '{{state}}'
    - name: pkg-config - install
      apt:
        name: pkg-config
        state: '{{state}}'
    - name: libglu1-mesa - install
      apt:
        name: libglu1-mesa
        state: '{{state}}'
    - name: libgtk-3-dev - install
      become: true
      apt:
        name: libgtk-3-dev
        state: '{{state}}'
    - name: liblzma-dev - install
      become: true
      apt:
        name: liblzma-dev
        state: '{{state}}'
    - name: libstdc++-12-dev - install
      become: true
      apt:
        name: libstdc++-12-dev
        state: '{{state}}'

</code></pre>
<h3>Configure a text editor or IDE</h3>
<p>In VS Code install Flutter extension. Recomemnded that you install VS Code version > 1.86.</p>
<p>The correct extension is published by Dart Code.</p>
<h3>Install the Flutter SDK</h3>
<p>There's a manual tar download option, but in this case I will use VS Code command pallete and follow the wizard.</p>
<h3>Create the project</h3>
<p>From command palette create a new Application and wait until it starts.</p>
<p>Very slow on the T480, but ultimately created a functional app.</p>
<p>Test the installation of Flutter using the <code>flutter doctor</code> command within VS Code terminal.</p>
<h2>Start developing apps with Flutter</h2>
<p>Use the following guides to continue learning about development with Flutter.</p>
<ol>
<li>
<p><a href="https://docs.flutter.dev/get-started/codelab/">Learn how to write first Flutter app</a></p>
</li>
<li>
<p><a href="https://docs.flutter.dev/get-started/fundamentals/">Flutter fundamentals docs</a></p>
</li>
<li>
<p><a href="https://docs.flutter.dev/platform-integration/linux/building">Build Linux apps with Flutter</a></p>
</li>
<li>
<p>Seems to be a lengthy video tutorial that is cross platform on Desktop and Web</p>
</li>
<li>
<p>Seems like a better outline, but still not clear</p>
</li>
</ol>
<p>One surprising thing to notice is that the Linux development side of Flutter is using C. A positive surprise, but unexpected. I thought I'd still be dealing with Dart.</p>
<h2>Summary</h2>
<p>Relatively straightforward install, but the development guide itself for Linux is lacking. Lots of piecing together, but seems interesting to learn about app development using C.</p>
<p>Will push on with this further and aim to build an RSS app for Linux.</p>

                ]]>
            </description>
        </item><item>
            <title>W3C - easy checks for accessibility</title>
            <link>https://alanionita.github.io/portfolio--svelte/blog/2025.02.10.checks-for-accessibility</link>
            <pubDate>Mon, 10 Feb 2025 00:00:00 Z</pubDate>
            <author>Alan Ionita</author>
            <description>
                <![CDATA[
                    <h1>W3C - easy checks for accessibility</h1>
<p>Accessibility is important and has been since working at <a href="https://www.em-code.com/">Code Computer Love</a>, a design and development agency from Manchester known for top notch Design and UX research.</p>
<p>Here is a rundown of the <a href="https://www.w3.org/WAI/test-evaluate/preliminary/">W3C - preliminary check</a> as applied to my new portfolio redesign.</p>
<h2>Title page</h2>
<p>Test:</p>
<ul>
<li>[ ] each page contains a title that is visible in the browser</li>
</ul>
<p>Test notes:</p>
<ul>
<li>Missing</li>
</ul>
<p>Implement:</p>
<ul>
<li>Create a dynamic title component and load it into each page.</li>
<li>This will allow for a dynamic title that always includes "Alan Ionita - " for search ranking purposes</li>
</ul>
<pre><code class="language-javascript">// PageTitle component

&#x3C;script>
    const siteName = "Alan Ionita"
    let props = $props();
&#x3C;/script>

&#x3C;svelte:head>
	&#x3C;title>{siteName + " - " + props.text}&#x3C;/title>
&#x3C;/svelte:head>

</code></pre>
<pre><code class="language-javascript">// *Page component
import PageTitle from '...'

&#x3C;PageTitle text="Each page title" />

</code></pre>
<h2>Page headings</h2>
<p>Test:</p>
<ul>
<li>[ ] The page has a heading. In almost all pages there should be at least one heading.</li>
<li>[ ] All text that looks like a heading is marked up as a heading.</li>
<li>[ ] All text that is marked up as a heading is really a conceptual section heading.</li>
<li>[ ] The heading hierarchy is meaningful. Ideally the page starts with an "h1" — which is usually similar to the page title — and does not skip levels; however, these are not absolute requirements.</li>
</ul>
<p>Test notes:</p>
<ul>
<li>HomePage: hierarchy is a bit odd with only "Alan Ionita," being marked up as h1; the whole section should be an h1 really</li>
<li>HomePage: rest of the hierarchy makes sense</li>
<li>HomePage: html structure for each section seems a bit overkill; too many nested elements</li>
<li>BlogPage: headings are good</li>
<li>BlogPostPage: markdown to html headings converted correctly, but needed to correct the source markdown to make sure that each post had an h1</li>
</ul>
<h2>Contrast ratio</h2>
<p>Test:</p>
<ul>
<li>[ ] minimum contrast: a contrast ratio of at least 4.5:1 for normal-size text.</li>
<li>[ ] provide support for high contrast</li>
</ul>
<p>Test notes:</p>
<ul>
<li>BlogPostPage: starting here, because it's the page with most content; core font is rated as AAA compliant with a contrast of 6.77;</li>
<li>BlogPage: h1 is compliant to AAA (6.77), blog list main text is AA (3.09) and sub-text AA (3.84)</li>
<li>HomePage: orange section is AA, rest is AAA; same scores as above</li>
<li>Nav: not-visited links are not compliant (2.9)</li>
</ul>
<p>Implement:</p>
<ul>
<li>Find colour variant for orange to get to AAA adherence</li>
<li>Missing support for "prefers-contrast" option, high contrast solution for users that need it. Will require some research on best colours to use to achieve a high contrast whilst still preserving the theme, as a fallback will just default to black,yellow combination</li>
<li><code>prefers-contrast</code> further reading <a href="https://hacks.mozilla.org/2020/07/adding-prefers-contrast-to-firefox/">here</a></li>
</ul>
<h3>Protocol: check contrast</h3>
<p>My preference is to use Firefox for personal development and will not cover Chrome.</p>
<ol>
<li>
<p>Inspect text element</p>
</li>
<li>
<p>Inspector: verify the correct html element is selected</p>
</li>
<li>
<p>Inspector: in the Styles side panel find the class that provides the color css</p>
</li>
<li>
<p>Inspector / Styles: click on the color round icon</p>
</li>
<li>
<p>Inspector / Styles: check the contrast in the pop-up</p>
</li>
</ol>
<h2>Text resize</h2>
<p>Test:</p>
<ul>
<li>[ ] make sure that resizing text doesn't cause page overflows, or stop interactive elements from being accessible</li>
</ul>
<p>Test notes:</p>
<ul>
<li>Overflowing was causing the Nav to no longer be accessible, so made a change to fix that.</li>
<li>All other elements and pages are passing.</li>
</ul>
<h2>Keyboard access and visual focus</h2>
<p>Test:</p>
<ul>
<li>[ ] test keyboard nav and visual focus</li>
</ul>
<p>Test notes:</p>
<ul>
<li>HomePage: keyboard navigation works correctly across links, but ideally the navigation should cover all the sections too; will require a refactor to improve the page</li>
<li>BlogPage: works as expected since the list is correctly styled as a list</li>
<li>BlogPostPage: correctly navigates through links, but this is correct behavior here</li>
<li>Tab all, Tab away, Tab order is fine</li>
<li>All functionality works by keyboard, although admittedly it's a very simple site with a low number of features</li>
<li>Dropdown list navigation and image links considerations are not applicable here</li>
</ul>
<p>Implement:</p>
<ul>
<li>Visual focus styling is a visually weak, somewhat clashing with the theme; will require a refactor</li>
</ul>
<h2>Forms, labels, and errors</h2>
<p>Not applicable here</p>
<h2>Moving, Flashing, or Blinking Content</h2>
<p>Test notes:</p>
<ul>
<li>Since the site is a static site, there's no perceivable load or layout shift.</li>
<li>The only thing of note here is the transition from <code>/</code> to <code>/blog</code>, where <code>/</code> is a dark colour and <code>/blog</code> is a lighter colour. Here the design is such that on <code>/blog</code> there's colour degradation from top to bottom of page so the user is gradually moving from dark colours to lighter colours.</li>
</ul>
<h2>Multimedia (video, audio) alternatives</h2>
<p>Not applicable</p>
<h2>Basic structure check</h2>
<p>Test:</p>
<ul>
<li>[ ] Turn off images and show the text alternatives.</li>
<li>[ ] Turn off style sheets (CSS), which specifies how the page is displayed with layout, colors, etc.</li>
<li>[ ] Linearize the page or the tables (depending on the toolbar).</li>
</ul>
<p>Test notes:</p>
<ul>
<li>HomePage: Images are not applicable, CSS off looks ok, already a fairly linear layout so working as expected; nav is off to right, should fix</li>
<li>BlogPage: same nav issue, main text ok; post list is completely white and invisible, should fix- BlogPostPage: passing, same nav issue</li>
<li>Both issues were down to poor testing, see the below script to correctly run the test</li>
</ul>
<h3>Protocol: Disabling CSS stylesheets</h3>
<p>Run the following in the Developer Tools Console</p>
<pre><code class="language-javascript">document.querySelectorAll('style, link[rel="stylesheet"]').forEach(e => e.remove());
</code></pre>
<h2>Summary</h2>
<p>With the exception of the orange colour and the visual focus, we have a AAA compliant website.</p>

                ]]>
            </description>
        </item><item>
            <title>Timeline apps - past, present, future</title>
            <link>https://alanionita.github.io/portfolio--svelte/blog/2025.02.09.timeline-apps</link>
            <pubDate>Sun, 09 Feb 2025 00:00:00 Z</pubDate>
            <author>Alan Ionita</author>
            <description>
                <![CDATA[
                    <h1>Timeline apps - past, present, future</h1>
<p>Social Media is a disaster. Under the guise of connecting with friends and belonging to a community there's just one driver: profit.</p>
<p>And in that chase for profit we have lost out on core hygiene elements that platforms should consider:</p>
<ul>
<li>moderation</li>
<li>stability</li>
<li>access</li>
<li>user control</li>
</ul>
<p>The future however might not be so bleak because of the increasing popularity of "timeline apps". To understand them I'd like to set the scene for the trend and them explain the apps themselves.</p>
<h2>Social Media advertising</h2>
<p>Social Media advertising spend in 2024 was $164bil [*1] globally, but after digging into the data and predictions it seems to be designed by hockey stick hopefuls not real analysts.</p>
<p>In the UK social media paid ads are classified under "Display" ads, and UK digital advertising industry body (IAB.UK) shows that in 2013, Display ads were worth 1.89 billion pounds [*2]. In 2024 the revenue [*3] from Display ads went up to 6.5 billion pounds.</p>
<p>Nice growth you'd say? Sure, "+22% YoY" the IAB.UK quotes. The interesting thing however is how that money is split: video and non-video. Non-video is social media and other ads, which in 2024 amounted to 2.5 billion pounds.</p>
<p>Anecdotally I remember social being up to 11% in 2013, so 0.207 billion pounds vs 2.5 billion pounds. 12x increase in 11 years. Again not bad!</p>
<p>The driver for the ad industry is the not necessarily consistent return, they're not pension funds. The ad industry "rides the wave" and so they need to follow and the hottest trends and deploy capital accordingly. The driver is maximum return on investment, and maxROI is gained from deploying capital with a weight towards top performing segments.</p>
<p>In 2024 that segment was not social, it was digital video. And by a long margin 2x. As an ad agent you cannot pass on that opportunity.</p>
<p>Which leaves social advertising with a massive deficit in capital deployment: they will get at minimum 1/3 of ad spends.</p>
<p>Social advertising is not making money where "making" is the exponential scale that most business crave for.</p>
<h2>RSS and the past</h2>
<p>Social media advertising is generally in decline today, mostly because of the financial view explained above and because of other smaller platform related issues.</p>
<p>In 2013 social media ads were growing and performing well. In 2011 Google launched Google Plus+ and by 2013 the performance of Google Plus+, social advertising meant that Google decided to kill one of their most loved products, Google Reader.</p>
<p>Google Reader was built on top of RSS, a protocol that stands for Really Simple Syndication. RSS allows a website to define what updates it has in the form of a feed. RSS Readers, like Google Reader would use these feeds to produce a custom feed for a user in a nice web interface.</p>
<p>Google Reader was nice and was the first "timeline app".</p>
<h2>Timeline apps today</h2>
<p>Today the big social media platforms are in trouble. They can't make revenue and most are adopting dark patterns to force us to share more details with a platform, whilst implementing harder and harder journey for exporting data and deleting accounts. A Facebook was notoriously hard to delete in 2013-2014, which is why people moved to Twitter.</p>
<p>Twitter, which started with a historical timeline, eventually moved to and "algorithm timeline" and pushed a lot of users and businesses to create solutions for this. <a href="https://en.wikipedia.org/wiki/TweetDeck">TweetDeck</a> was one such solutions and Twitter was relentless in making API changes that affected consumers like TweetDeck. Eventually Twitter bought TweetDeck and is not part of X Pro.</p>
<p>There are endless of other stories regarding social media companies acting like cartels and protecting the data within their platform and making it as painful as possible for anyone to development improved UIs that accessed that data. The reason is advertising.</p>
<p>RSS however bypasses that model and most websites on the web have one. RSS feeds are particularly popular on the IndieWeb, but also podcasts have RSS feeds, Youtube channels have RSS, even the new generation of federate social media has them Mastodon and it's famous cousin Bluesky.</p>
<p>Timeline apps use RSS to pull all this information into one UI.</p>
<p><a href="https://apps.apple.com/gb/app/feeeed-rss-reader-and-more/id1600187490">feeeed</a> for example is a fantastic app for a custom timeline. In addition to news, you can add any website or newsletter and you can even connect to Health  track your walking goals in the app. Happy to report that no ads are present yet.</p>
<p><a href="https://apps.apple.com/us/app/tapestry-by-iconfactory/id6448078074">Tapestry</a> is another good timeline app, but they've launched with ads baked in.</p>
<p>Whilst there are more examples out there, these above are the "latest" incarnation of the timeline app. Sure we still have tradition RSS readers which function similarly, but the slick UI and extra focus on the timeline is not there.</p>
<p>Timeline apps will more of a focus in the future, with more people choosing them other a locked in platform. Facebook for example is already moving towards an open platform with Threads and Instagram, but it remains to be seen if they will continue or lock everything down.</p>
<p>Should we have ads in timeline apps? In my honest opinion, no. The user space is more private and more self-aware, so ads should fade to subscriptions or donations.</p>
<p>We must learn from the lessons of the 2013-2024 social web and keep our data. Media empires should not be built on top our data, but rather small apps should be supported because they give us value with being absolutely extractive.</p>
<p>Give timeline apps a try and better still, go and make one!</p>
<p>References:</p>
<ul>
<li>
<ol>
<li>Social media advertising spend in 2024, <a href="https://www.statista.com/statistics/271406/advertising-revenue-of-social-networks-worldwide/">link</a></li>
</ol>
</li>
<li>
<ol start="2">
<li>IAB.UK 2013 advertising budgets, <a href="https://www.iabuk.com/sites/default/files/research-docs/IAB_PWC_AdspendOnePager_2013.pdf">link</a></li>
</ol>
</li>
<li>
<ol start="3">
<li>IAB.UK 2024 advertising budgets, <a href="https://www.iabuk.com/sites/default/files/public_files/UK%20Digital%20Adspend%20H1%202024%20Update.pdf">link</a></li>
</ol>
</li>
</ul>

                ]]>
            </description>
        </item><item>
            <title>Thinkpad T480 screen flicker fix</title>
            <link>https://alanionita.github.io/portfolio--svelte/blog/2025.02.07.t480-screen-flicker</link>
            <pubDate>Fri, 07 Feb 2025 00:00:00 Z</pubDate>
            <author>Alan Ionita</author>
            <description>
                <![CDATA[
                    <h1>Thinkpad T480 screen fix on X11</h1>
<p>It's unclear to me why, but Linux Mint 22.1 on the T480 causes screen flickering.</p>
<p>To experience the issue check the login screen once the laptop has come out of suspend, or open a dock applet and pause on a dropdown for a long time.</p>
<p>Issue seems to be related to transparency and screen sleep.</p>
<h2>How to Fix</h2>
<ol>
<li>Check that <code>mesa-utils</code> package is installed</li>
</ol>
<pre><code class="language-shell">apt list | grep "mesa-utils"
</code></pre>
<p>Install the package if not present</p>
<ol start="2">
<li>Add this file to X11 config</li>
</ol>
<p>Open the X11 config directory</p>
<pre><code class="language-shell">cd /etc/X11/xorg.conf.d
</code></pre>
<p>Create the <code>20-intel.conf</code> within the above directory, with the following contents</p>
<pre><code class="language-shell">Section "Device"
 Identifier "Intel Graphics"
 Driver "Intel"
 Option "AccelMethod" "sna"
 Option "TearFree" "true"
EndSection
</code></pre>
<ol start="3">
<li>Restart the computer</li>
</ol>
<hr></hr>
<p>You can also fix it by installing KDE Plasma :P /s</p>

                ]]>
            </description>
        </item><item>
            <title>A computer for life</title>
            <link>https://alanionita.github.io/portfolio--svelte/blog/2025.02.07.computer-for-life</link>
            <pubDate>Fri, 07 Feb 2025 00:00:00 Z</pubDate>
            <author>Alan Ionita</author>
            <description>
                <![CDATA[
                    <h1>A computer for life</h1>
<p>It's easy to think about computers today as always present on our person, the smartphone. They're useful for many things, but addictive in equal measures.</p>
<p>Strangely alonside this new addiction side-effect computers today have another side-effect we don't notice: price.</p>
<p>Price is a proxy for value and if we get more value I'm all for increased price. Sadly the trend today is to get less value, less use out of a computer that somehow asks for a higher price.</p>
<p>AI has now made this worse. As an industry AI is vastly a service business, rent what compute you need vs a DIY gig, and so it follows that computers with AI cost more.</p>
<p>But they don't just have a higher outlay, they also have a higher recurring expense via subscriptions - a topic I might write on later.</p>
<p>For the sake of the argument I wanted to outline the marketing reasons for computers and why they are unnecesary.</p>
<h2>Design</h2>
<p>Design is valuable and design is important. I'm a huge fan of nice looking and functioning objects, but neither I not Deiter Rams would disagree that good design is a synonim to expensive design.</p>
<p>Soviet design for example is somewhat brutal, stark, bold, yet cheap and easy to mass produce.</p>
<p>Yet today in 2025, computer designs warrant a high price tag.</p>
<p>Why? Because... we buy it and buying gives businesses the signal that this is what we want and that they can charge more.</p>
<h2>Repairability</h2>
<p>We've all had computers fail and require repairs. Repairs are not chear, the computer itself was not cheap and we're constantly worried about what happens with our device once it's at a service. What happens to our data?</p>
<p>Sadly, the newer the computer and the "better" the less repairable it is.</p>
<p>The cause for this is tied to design:</p>
<p>Business wants nicer design -> They posit that "nice" = "small" -> Smaller devices means less space inside for accessible mounts, more use of tape and glue, less room for device saving features like (liquid draining)</p>
<p>But therein lies the problem, a company design is subjective. Over time it can become a universal looks for the object, but initially it's a choice we're given.
So I ask myself why would I want a device that is harder to repair?</p>
<h2>UX</h2>
<p>We all remember the Apple butterfly keyboard incident from a few years back? The summary is that Apple in their chase for smaller, redesign the keyboard switches, the new design felt weird and was prone to dust damage. Yes you read that right, the if tiniest bit of dust would get under the key you're done for.</p>
<p>Yet millions of these buterfly keyboard computers sold at a minimum price of ~800 GBP.</p>
<p>The public will bought them even though you were getting a laptop that was not as nice to use day to day.</p>
<p>The removal of headphone jacks and ports is another UX faux pas, leaving the public stuck with dongles and somehow prices are going up.</p>
<h2>Software</h2>
<p>Windows is the classic culprit for the idea of bad software that sells well. But lately we see more and more companies "skin" the os or loading up new computer with pre-packaged software and in some cases pre-packaged malware.</p>
<p>Apple for example has a trained skill in producing new versions of Mac OS that are locked in step with the newest hardware, leaving a customer with an older device with a noticeably slower device.</p>
<p>They incentives with software is to make more money, whether from forcing more future purchases or from data collection.</p>
<p>Android is no exception to this with Google services being know to send user data to Google in support of their ad delivery platform.</p>
<h2>Today's computer</h2>
<p>In other words today's computer is not a computer for life. We've grown acustomed to believing and buying into the idea that a computer is both a high-price item and an item that you should replace at regular intervals.</p>
<p>A computer is not a stable platform for work, but a source of change because big manufacturers want to experiment, cut manufacturing costs, increase ad revenue.
But to all this negativity there's a potential solution</p>
<h2>Alternatives</h2>
<p>Before I describe the alternatives I need to start with the software. Software is what allows us that stable platform, that universal common denominator across hardware. Although not perfect by any means Linux is the best option we have right now for a stable software base.</p>
<p>Core features like browing the web, word processing, playing games is available across all flavours of Linux and the experience of using 1 flavour vs the other is relatively unchanged.</p>
<p>On the mobile software side iOS is locked and Android is not, so the option is clear. But stock Android comes with Google services so we only have 2 options: either a De-Googled Android, or mobile Linux.</p>
<p>Mobile Linux is sadly still being developed on and is only stable on a handful of phones. You also loose out on any Android apps and have to depend on the browser for banking etc.</p>
<p>Of the De-Googled Android flavours, <a href="https://grapheneos.org/">GrapheneOS</a> is the best example of a simple os that mimicks Android whilst kicking out Google services and still maintaining security.</p>
<hr></hr>
<p>With the software in place we can now consider some hardware.</p>
<p>To a degree the software will have an impact on the hardware. Linux is largely supported, but newer hardware is more likely to cause issues. Equally really old hardware can become unsupported by the kernels.</p>
<p>Thinkpads are by far the best computer for the money.</p>
<p>Thinkpads are cheap on the used market. They feature good hardware that is usually upgradeable. They feature easy to repair designs and detailed manuals on how to replace and troubleshoot any issue.</p>
<p>Thinkpads also offer the best keyboards on the market. Keyboards that have largely remained unchanged and constant.</p>
<p>You're computer will always be a stable unchanging tool with a Thinkpad.</p>
<p>As for phones, the Pixel phones are the best option because most of security updates are made in the open and GrapheOS can use these to maintain the security of their OS.</p>

                ]]>
            </description>
        </item><item>
            <title>Testing - Code syntax highlighting</title>
            <link>https://alanionita.github.io/portfolio--svelte/blog/2024.07.27.code-highlight</link>
            <pubDate>Sat, 27 Jul 2024 00:00:00 +0100</pubDate>
            <author>Alan Ionita</author>
            <description>
                <![CDATA[
                    <h1>Testing - Code syntax highlighting</h1>
<p>Currently only supports: Javascript, Yaml, Shell</p>
<p>Needs expanding for every new language.</p>
<h2>Javascript</h2>
<pre><code class="language-javascript">const x = 10
const y = 5

function add(a, b) {
    return a + b
}

const result = add(x, y);
</code></pre>
<h2>Yml</h2>
<pre><code class="language-yml">name: E2E Cypress testing
on:
  push:
    branches:
      - main
jobs:
  cypress-run:
    runs-on: ubuntu-20.04
    steps:
      - name: Checkout
        uses: actions/checkout@v2

      - name: Cypress run
        uses: cypress-io/github-action@v2
        with:
          browser: chrome
          headless: true
          env: host=https://alanionita.github.io
          
      - uses: actions/upload-artifact@v1
        if: failure()
        with:
          name: fails
          path: cypress/    
</code></pre>
<h2>Yaml</h2>
<pre><code class="language-yaml">name: E2E Cypress testing
on:
  push:
    branches:
      - main
jobs:
  cypress-run:
    runs-on: ubuntu-20.04
    steps:
      - name: Checkout
        uses: actions/checkout@v2

      - name: Cypress run
        uses: cypress-io/github-action@v2
        with:
          browser: chrome
          headless: true
          env: host=https://alanionita.github.io
          
      - uses: actions/upload-artifact@v1
        if: failure()
        with:
          name: fails
          path: cypress/    
</code></pre>
<h2>Shell</h2>
<blockquote>
<p>Doesn't seem to be displaying correctly but works</p>
</blockquote>
<pre><code class="language-shell">
cat ./test/test-file.txt

wc -l en_US.twitter.txt

foo@bar:~$ whoami

foo

</code></pre>

                ]]>
            </description>
        </item><item>
            <title>Life OS</title>
            <link>https://alanionita.github.io/portfolio--svelte/blog/2021.03.28.life-os</link>
            <pubDate>Sun, 28 Mar 2021 00:00:00 Z</pubDate>
            <author>Alan Ionita</author>
            <description>
                <![CDATA[
                    <h1>Life OS</h1>
<p>After recently listening to <a href="https://www.calnewport.com/podcast/">Cal Newport's - Deep Questions podcast</a> I decided to think more about my system for managing life goals.</p>
<p>The system suggests the following docs:</p>
<ul>
<li>core plan or root plan</li>
<li>personal values plan</li>
<li>professional values plan</li>
<li>maintenance plan</li>
<li>progress plan</li>
</ul>
<p>This docs is my personal interpretation of that system.</p>
<h2>Why plan?</h2>
<p>The reality is that life is measured in years, yet lived by the day, hour, minute, second.</p>
<p>Life expectancy is up so in my situation, if I'm lucky, I can expect a good 50 years of ... life!</p>
<p>50 years is a looooooong time without a map of what you care about, what you want to do, where you want to go, and who you want to be.</p>
<p>To some planning might remove some of the fun from the next few years. I've recently reread the <a href="https://waitbutwhy.com/2015/12/the-tail-end.html">The tail end</a> article from WaitButWhy to make me realise that I plan not to decide what I want to happen, but to make sure that everything I want does happen.</p>
<p>Professionally, software development is and ever evolving field and as an engineer I need to chart the best course into the future, whilst making sure I'm still relevant now.</p>
<h2>The ever evolving plan</h2>
<p>I've used a few methods to carve out this plan in the past. I wanted to outline the resources and current plan I've used so far.</p>
<p>The one method that I've abused a lot over the years was 'do more of what you like', which explain why I spent more time playing video games, that studying in high school.</p>
<p>Should we do more of what we like or should we find areas where that selfish goal also has a an altruistic side-effect.</p>
<p>This is where another great resource came in for me, <a href="https://80000hours.org">80000hours.org</a>. The site highlights the greatest issues facing humanity and informs the reader how to pursue the biggest impact jobs working on those issues.</p>
<p>I use the guidance from 80000h as a moral compass that allows me to gauge if I'm making an impact.</p>
<p>For planning I tend to run yearly goals, determined annually in June. I use the <a href="https://yearcompass.com">YearCompass </a> framework to help me reflect on the past year and chart the next.</p>
<p>For day to day operation I use a Bullet Journal system. Every month I'll use my yearly goals to set monthly aims, and then I daily / weekly aim to spend time towards those goals.</p>
<p>I usually timebox things in the calendar in order to make sure I do them or spend the time.</p>
<p>Another important concept to mention are distractions. My aim is to constantly limit distraction to allow more time for working on the plan.</p>
<h2>Root plan</h2>
<p>A document where core systems and strategies to organise life are stored. Instead of storing everything in your head - write it down, flesh it out.</p>
<p>This plan must be generic and should not capture the specific rules.</p>
<p>The <code>root</code> will point to other more specific documents:</p>
<h2>Value plan</h2>
<p>An outline of roles responsibilities and the core values I need to display in those.</p>
<h2>Personal plan</h2>
<p>A document on all personal goals and processes.</p>
<h2>Professional plan</h2>
<p>A document on all professional goals and processes.</p>
<h2>Maintenance procedure</h2>
<p>How do I maintain these systems - when, where, what.</p>
<p>A good idea here is to think about how data enters the system and how it leaves it.</p>
<h2>Operational plan</h2>
<p>How do things get done and what are some of the habits and process you use to achieve the things on the plan.</p>
<h2>Metrics</h2>
<p>What are you going to track in order to measure the system.</p>

                ]]>
            </description>
        </item><item>
            <title>Raspberry Pi OS Study</title>
            <link>https://alanionita.github.io/portfolio--svelte/blog/2021.03.21.pi-os-study</link>
            <pubDate>Sun, 21 Mar 2021 00:00:00 Z</pubDate>
            <author>Alan Ionita</author>
            <description>
                <![CDATA[
                    <h1>Raspberry Pi OS Study</h1>
<p>I use the Raspberry Pi a lot, they are great little devices for prototyping ideas and building projects.</p>
<p>Choosing an OS has been tricky because there are so many options out there.</p>
<p>My needs are usually simple:</p>
<ul>
<li>[ ] easy headless setup (no monitor, keyboard, or mouse needed)</li>
<li>[ ] reliable OS</li>
<li>[ ] flexible OS</li>
<li>[ ] transferable skills</li>
<li>[ ] ROS support*</li>
<li>[ ] small footprint</li>
<li>[ ] solid support</li>
</ul>
<p>*This is a new requirement that has limited my choices a lot.</p>
<h2>What is my process</h2>
<p>I've recently started using <a href="https://www.ansible.com">Ansible</a> to run a set of commands on the Pi that can harden the OS and improve the security.</p>
<p>In general I just care about an OS that can take minutes to flash and is easy to SSH into via Ansible once connected to my network.</p>
<p>I'm planning to write a post on Ansible soon.</p>
<h2>What is the hardware?</h2>
<p>I prefer the standard RPi unit usually the 4GB or 8GB version.</p>
<p>Although I've tried the Pi Zero, I've not used it for projects recently. I would use it if I need the form factor, but using it will probably make me change the OS.</p>
<h2>What was I using?</h2>
<p><a href="https://blog.hypriot.com">HypriotOS</a>, which is an OS that comes with Docker pre-installed.</p>
<p>Hypriot was also really easy to configure before mounting - it came with a clear documentation and the config files were easy to get to.</p>
<p>Unfortunately the HypriotOS community and support is small at the minute and I've encountered a few issues with the OS.</p>
<p>I wanted to move away from it before support became a problem.</p>
<ul>
<li>✅ easy headless setup (no monitor, keyboard, or mouse needed)</li>
<li>🟡 reliable OS</li>
<li>✅ flexible OS</li>
<li>🟡 transferable skills</li>
<li>❌ ROS support*</li>
<li>✅ small footprint</li>
<li>❌ solid support</li>
</ul>
<h2>What's wrong with Raspberry Pi OS?</h2>
<p>Raspberry Pi OS (Raspian) was harder to set up in headless mode a few months ago. In my case I remember there being a lot of steps in order to set everything up.</p>
<p>I should test this again, but since my main requirement is not met (ROS support) I might not use it anyway.</p>
<ul>
<li>🟡 easy headless setup (no monitor, keyboard, or mouse needed)</li>
<li>✅ reliable OS</li>
<li>✅ flexible OS</li>
<li>🟡 transferable skills</li>
<li>❌ ROS support*</li>
<li>✅ small footprint</li>
<li>✅ solid support</li>
</ul>
<h2>OS Study</h2>
<h3><a href="https://ubuntu.com/download">Ubuntu Server LTS 20.04</a></h3>
<p>Best option with a small footprint (until you update/upgrade). Support with ROS and Ubuntu itself is a popular OS in general which makes troubleshooting easy.</p>
<p>LTS stands for Long Term Support so I can reliably reuse this image for projects for the next few years.</p>
<p>This also means that it will be a popular option for enterprise clients too so all the skills learned are transferable, which I can't say the same about Raspberry Pi OS or HypriotOS.</p>
<p>The setup is simple because Ubuntu Server is included in the <a href="https://www.raspberrypi.org/software/">Raspberry Pi Imager</a>.</p>
<ul>
<li>✅ easy headless setup (no monitor, keyboard, or mouse needed)</li>
<li>✅ reliable OS</li>
<li>✅ flexible OS</li>
<li>✅ transferable skills</li>
<li>✅ ROS support*</li>
<li>🟡 small footprint</li>
<li>✅ solid support</li>
</ul>
<p>Not the smallest OS on the list, especially after updating and upgrading.</p>
<p>Result: best pick for now</p>
<h3><a href="https://alpinelinux.org/about/">Alpine Linux</a></h3>
<p>Huge fan of Alpine from Docker and I really wanted to make this work as my server driver.</p>
<p>Again the ROS requirement is a 'no-go', but for other project this could be an ideal OS.</p>
<p>The size and performance is awesome!</p>
<p>Skills learned through using the OS are not as transferable as Ubuntu, but you can reuse those skill if you ever deal with Docker instance - which is still a plus.</p>
<p>The setup on the site is a lot more complex that what I came up with</p>
<pre><code class="language-shell">// 1. Download the tar from the Alpine site

// 2. Create a bootable SD card using PI imager

// 3. Extract the tarball contents to the SD card.

tar -xvf alpine-rpi-3.13.2-aarch64.tar.gz -C {SD-card}

// 4. Pop the SD card into the Pi 
</code></pre>
<p>Annoyingly I need a desktop and peripherals in order to complete the setup.</p>
<ul>
<li>❌ easy headless setup (no monitor, keyboard, or mouse needed)</li>
<li>✅ reliable OS</li>
<li>✅ flexible OS</li>
<li>✅ transferable skills</li>
<li>❌ ROS support*</li>
<li>✅ small footprint</li>
<li>✅ solid support</li>
</ul>
<p>Result: cannot complete the setup in headless mode</p>
<h2><a href="https://ubuntu.com/core">Ubuntu Core</a></h2>
<p>An OS specifically designed for embedded solutions.</p>
<p>Core has a lot of interesting features for embedded: OTA updates, secure boot, full disk encryption, recovery mode, and Snaps.</p>
<p>Everything apart from Snaps is awesome and as an ex-embedded developer I was drooling when I read about it.</p>
<p><a href="https://ubuntu.com/core/docs/snaps-in-ubuntu-core">Snaps</a> are 🟡 for me. A lot of people are strongly against it, but I'm not one of them.</p>
<p>I can live with the format, but it does introduce a new step in the workflow: compiling your app to a Snap.</p>
<p>After a bit of reading I discovered the official Canonical advice re: Ubuntu Core:</p>
<blockquote>
<p>To be used as part of a set and forget system. For deployment please use Ubuntu Server</p>
</blockquote>
<p>That makes sense now and it's a serious contender for finished projects.</p>
<p>The setup is simple because Ubuntu Core is included in the <a href="https://www.raspberrypi.org/software/">Raspberry Pi Imager</a>.</p>
<p>Annoyingly though Ubuntu Core requires a desktop and peripherals in order to complete the setup.</p>
<ul>
<li>❌ easy headless setup (no monitor, keyboard, or mouse needed)</li>
<li>✅ reliable OS</li>
<li>❌ flexible OS</li>
<li>❌ transferable skills</li>
<li>❌ ROS support*</li>
<li>✅ small footprint</li>
<li>✅ solid support</li>
</ul>
<p>I marked it down for flexibility because Ubuntu Core is not meant for development. Also marked it down for transferable skills since it seems like there's a lot of specific Ubuntu Core knowledge needed to make this work and those skills are not all transferable.</p>
<p>Result: cannot complete the setup in headless mode</p>
<h2><a href="https://github.com/skiffos/skiffos">SkiffOS</a></h2>
<p>Skiff is a lightweight OS and is very similar to HypriotOS.</p>
<p>It's built around Docker using Buildroot to produce a minimal 'single-file' host OS. 🤯</p>
<p>Another benefit here was the fact that I can configure the build before compilation so I can probably reduce the amount of work done on the machine to complete the setup.</p>
<p>The config files use <code>yaml</code> which is a super transferable skill in Docker and Cloud environments.</p>
<ul>
<li>🏁 easy headless setup (no monitor, keyboard, or mouse needed)</li>
<li>🏁 reliable OS</li>
<li>✅ flexible OS</li>
<li>✅ transferable skills</li>
<li>❌ ROS support*</li>
<li>✅ small footprint</li>
<li>🏁 solid support</li>
</ul>
<p>The setup seems easy enough but I had issues with the gcc and make so I couldn't actually try this out properly.</p>
<p>The fact that it comes with Docker pre-installed makes this a really 🌶 option for flexibility. It's also got a really small footprint depending on which layers you need and how many.</p>
<p>🏁 means that although I started testing, I couldn't make my mind just yet. SkiffOS is a new project so I won't be able to tell how they are with future support just yet.</p>
<p>Result: needs more testing in order to get a clear view</p>

                ]]>
            </description>
        </item><item>
            <title>What Next</title>
            <link>https://alanionita.github.io/portfolio--svelte/blog/2021.03.19.what-next</link>
            <pubDate>Fri, 19 Mar 2021 00:00:00 Z</pubDate>
            <author>Alan Ionita</author>
            <description>
                <![CDATA[
                    <h1>What next?</h1>
<p>In my <a href="https://www.amazon.co.uk/Pragmatic-Programmer-Andrew-Hunt/dp/020161622X">Pragmatic Programmer</a> the authors talk about your 'learning portfolio'. The assumption being that you should treat your learning like an investment portfolio and invest your time in topics depending on risk.</p>
<p>2021 is my 4th year as a developer and I've been processing my learning portfolio a lot lately.</p>
<h2>Summary</h2>
<p>My current learning portfolio is made up from high risk areas (embedded, machine learning, robotics), medium risk (web security), and low risk (web frameworks, web native, AWS cloud).</p>
<h2>What is my risk?</h2>
<p>Risk tolerance is unique to everyone. For me I'm in a high risk stage at the minute.</p>
<p>This means that I can afford to explore a lot and not really care about consequences to my career.</p>
<p>I actually don't see programming as a career, it more of a profession - it something I myself doing for the rest of my life. The fact that I get paid for it is a really nice bonus.</p>
<pre><code class="language-shell">risk = time_investment / personal_gain;
</code></pre>
<p>Risk has nothing to do with fun and it's mainly related to return - can I make a living or make money from it.</p>
<p>Choosing a high risk learning area is unlikely to payoff now, but it might return a bigger reward in the future as technology progresses.</p>
<p>High risk areas are more likely to be harder topics or use knowledge that you don't necessarily have right now.</p>
<h2>My Portfolio</h2>
<h3>Modern JS frameworks (low risk)</h3>
<p>React is a preference for me and I've made a salary from React since the beginning.</p>
<p>I know enough of React now that it doesn't take a lot of time for me to keep up with releases, new patterns, and features.</p>
<p>The time I've spent in the ecosystem has allowed to encounter problems with React performance, and tooling.</p>
<p>Right now my favourite way to write React is within NextJS, library that still uses React for the design whilst adding some nice features for Server-Side Rendered apps and static file generation. NextJS is flexible enough to allow a lot of config, but they provide conventions around common sticking points with React.</p>
<p>Following that pattern I'm also really excited by NestJS, which looks to do the same for Node / Express apis.</p>
<h4>Notable mentions</h4>
<p>I care about the JAMstack because I like the performance benefits. I keep an eye on new releases and I read tutorials for Eleventy and RedwoodJS. Same goes for JAM platforms: Zeit, Netlify.</p>
<p>I also like VueJS as a framework and I admire their performance focus and the fact that it's a community-driven project. I'm not interested in moving to VueJS just yet, so my interest is shallow - reading and tutorials.</p>
<p>I found React to be 'good enough' from a performance perspective (only relative to European customer bases), but as I'm looking into embedded websites I'm starting to research lighter weight options. Preact is a clear option here since there's little rework required. Also considering and testing Svelte, but it comes with a bit of a refactor from React.</p>
<h3>Web Native and DIY (low risk)</h3>
<p>This is a preference and an area of learning.</p>
<p>My preference is to DIY something over using a module - when I use a module it really has to be a flywheel.</p>
<p>When it comes to frontends, I tend to lean towards building everything from scratch. My experience with accessibility has force me down this path and I find it more flexible than reusing components from a library and trying to edit them.</p>
<p>This also means that I need to keep tabs on browser API releases, Javascript language releases, CSS releases and so on.</p>
<h3>AWS Cloud (low risk)</h3>
<p>Fullstack development requires knowledge on how to build architectures. My preference here is AWS and I'm currently working through certifications on AWS.</p>
<p>This actually covers a lot of other topics: databases, AI services, hosting services, server management security and so much more. I categorise all of that together because it's easier to track.</p>
<h3>Embedded Development (high risk)</h3>
<p>Learning more about FPGA and micro-controller development. Currently learning about building drivers for sensors, algorithms for measuring uncertainty, and building multi sensor systems.</p>
<h3>Machine Learning (high risk)</h3>
<p>Having already spent a few years working with machine learning algorithms, it's still something I'm fascinated about.</p>
<p>It took a while to realise what I enjoy within the machines learning area and right now I'm excited about planetary observation using AI.</p>
<p>A recent example of this was using satellite imagery to detect economic well-being in rural India.</p>
<p>Object detection is my main passion here, but also interested in autonomous vehicles.</p>
<h3>Robotics (high risk)</h3>
<p>Embedded development and machine learning folds into robotics.</p>
<p>The main reason why I'm interested in robotics is to do with robotics aids or using robots to speed up a traditionally human based workflow.</p>
<p>One of the earliest robotics projects I planned was a farming drone that can detect plant growth issues.</p>
<h3>Web Security (medium risk)</h3>
<p>During the pandemic I start getting interested in cyber-security.</p>
<p>For my learning I'm currently learning everything about web security vulnerabilities, exploits, and tooling.</p>
<p>The main reason for this learning areas is to apply it to bug bounties and vulnerability disclosures.</p>

                ]]>
            </description>
        </item><item>
            <title>Why Teach</title>
            <link>https://alanionita.github.io/portfolio--svelte/blog/2021.03.18.why-teach</link>
            <pubDate>Thu, 18 Mar 2021 00:00:00 Z</pubDate>
            <author>Alan Ionita</author>
            <description>
                <![CDATA[
                    <h1>Why Teach</h1>
<p>I've been asked this question a lot over the years, but never really dug into why.</p>
<p>Whenever I've been asked about it I've just shrugged and confirmed that I just like it. And that answer works for most conversations, but really - why do I like it?</p>
<h2>Lessons learned from teaching</h2>
<ul>
<li>teaching/mentoring others is hard because everyone is different - your role as a mentor is to quickly figure out the person and their learning style, and adjust your delivery</li>
<li>through teaching you learn more - the more people you teach the more you have access to different angles of looking at a problem and if you pay attention you'll learn something from that</li>
<li>teaching is humbling - you might not know the answer so it makes you realise how much more there is to learn</li>
<li>mentoring / teaching is rewarding - you get to see aspiring developers grow to junior developers in real time</li>
<li>teaching improves your memory of core basics, core basics improve your overall systems knowledge, and improved systems knowledge helps you with architecting systems and troubleshooting</li>
<li>your communication improves through teaching because of repetition and explaining the same concepts in different ways</li>
</ul>
<h2>My Teaching Timeline</h2>
<p>How long have I mentored or taught or advised on various topics?</p>
<p>During university I had chance to do a work placement in corporate sales. Up to then, my big dream was to be a CEO and move to Japan. My role was to sell HR and legal insurance so a big part of the pitch was fact finding about the business and seeing if there could areas where my coverage packages could help.</p>
<p>I very rarely oversold the package and was still the best in my hiring group. I cared about businesses and really wanted to learn about every single one and see where the product could actually help. A great job in hindsight for a business management student with a love of strategy.</p>
<p>That placement changed me for the better. I realised soon after that I wanted to make a difference so I continued advising entrepreneurs on marketing strategy during my last year of uni. I did it pro-bono, because I cared about exposure and the experience.</p>
<p>Some of these relationships turned out great and some didn't, but it was a great use of time.</p>
<p>I stopped doing all this pro-bono consulting once I graduated from uni and got into my first job, as mixed product/project manager at a small digital agency.</p>
<p>I managed the agencies retainer client so maintaining the client relationships was key. I achieved this by learning about their business, learning about their problem, and then figuring out what data I can find to plan out the solutions. It's unusual for a project manager to do this, which is why I talk about it as a mixed product/project role.</p>
<p>I was mentoring the clients on tech, strategy, and marketing.</p>
<p>I got back to my own strategy consulting a few years later whilst I was in my last season as a product manager. I started working with an incredible group of engineers developing aquaponics tech and sensors.</p>
<p>I couldn't code then, but knew enough about tech systems to know where to start. I was advising them on marketing, funding, strategy, whilst wanting to implement AI into the system. I wish I knew then how difficult a task that would be, but it got me to work with the Raspberry Pi for the first time.</p>
<p>My consulting was doing great: the group got selected as a contender for the Pitch@Palace event ran by a member of the Royal Family and I got to pitch the project to a room of investors and journalists.</p>
<p>But whilst I was mentoring the engineers on marketing they would teach me how to solder, how to code, how sensors worked and soon after that project reached it's completion I started thinking that I could actually program for a living.</p>
<p>I joined a coding bootcamp shortly after. I enjoyed the bootcamp environment so much that I jumped at the opportunity to teach a beginners class. I also applied for the instructor role, but wasn't as successful.</p>
<p>After graduating the bootcamp I was determined to continue with the tech mentoring so when the opportunity to start a coding bootcamp for refugees came up I was the first to sign up. I was a co-organiser of that bootcamp for a year and oversaw recruitment, teaching, and wellbeing for the students in collaboration with a team of like-minded mentors.</p>
<p>Some of the social problems facing refugees were getting to me and at that time I started to attend other mentoring groups that were casual and less frequent.</p>
<p>I kept doing this free tech mentoring for two years and in the process I must've worked with 100+ people looking to learn how to code.</p>
<p>Towards the end of that period I also got approached by a local university to help as a teaching instructor on their upcoming bootcamp.</p>
<p>This lead to me becoming and instructor on a future cohort.</p>
<h2>Conclusion</h2>
<p>I like teaching / mentoring for selfish reasons: it makes me a better developer and it also allows me to help aspiring developers join the industry.</p>

                ]]>
            </description>
        </item><item>
            <title>AWS - Serverless Advice</title>
            <link>https://alanionita.github.io/portfolio--svelte/blog/2021.01.27.serverless-advice</link>
            <pubDate>Wed, 27 Jan 2021 00:00:00 Z</pubDate>
            <author>Alan Ionita</author>
            <description>
                <![CDATA[
                    <h1>AWS - Serverless Advice</h1>
<p>I work for an AWS Advanced Partner. All of my work is inside AWS and most of the code and services we build and use are serverless.</p>
<p>My experience with serverless is a lengthy one, with the first time dipping my toe in the ecosystem in 2017 just as I graduated from a coding bootcamp.</p>
<p>Since then serverless has matured a lot, but I still see and hear of developers and businesses who haven't tried it yet or don't see the point.</p>
<p>With this post I'd hope to highlight why you should learn it, what you gain from it, and how to learn it (resources).</p>
<h2>Why you should learn it</h2>
<p>Serverless is popular because of the developer experience. It is so much easier to use a serverless service than a server-based deployment.</p>
<p>All the cloud vendors have incredible support for serverless: AWS, GCP, Azure.</p>
<p>New vendors are doing some really interesting work on serverless: Cloudflare, Netlify, Vercel.</p>
<p>These companies are at that stage in their growth where most of the services are free or close to free.</p>
<p>Tooling is incredible on all platforms. Even though no platform is awesome overall, each have their own gems - Azure is spending a lot of time on developer experience.</p>
<p>Serverless is simple to write. Everything is a function so by nature developers start to write small flexible micro-services. Part of the complexity of serverless comes from the architecture part of how we orchestrate all of the services to work together, but a quick refresher in distributed systems will help you navigate that.</p>
<p>Serverless is 'usually' cheaper that servers or virtual machines because of the on-demand nature and price per request.</p>
<p>Serverless is heavily documented. I'll takes AWS as an example and there are guides upon guides available. Not to mention official video, code snippets, and tutorials. That's just the official stuff too!</p>
<p>More and more companies require some knowledge. Due to the reasons above companies have adopted serverless and have either all their infrastructure as serverless solutions or run a hybrid mix of server and serverless solutions.</p>
<h2>How to learn it</h2>
<p>Serverless can be done through the platform consoles sure, but as soon as you start creating 2 or more resources the console becomes cumbersome, not to mention how hard it is to trace your steps.</p>
<p>It’s worth picking an infrastructure as code framework before you start and the best templating in my opinion is Serverless Framework or Terraform (courses below). Serverless Framework is easy to get started with, but there are some services that are not covered by it.</p>
<p>If you’re going serverless, go all in. If you have an existing architecture then port everything to serverless including DBs. It might need some refactoring, but moving slowly might not be as beneficial as you think. Event a mock fullstack sample will teach you more than gradually migrating the stack over time.</p>
<p>DynamoDB is the most common serverless database, but there are new ones coming into the fold. DynamoDB follows a concept of ‘single-table’ design which can be kinda hard to understand at first if you're used to SQL. You can sinply use different tables for entities event thought it means you loose some benefits and performance - single table designs are optimised for performance and cost.</p>
<p>Spend some time really digging into Lambda or Cloud Functions and how they function. Recent new features allow you to generate custom lambda containers using Docker - this really changes Lambda from a swiss-army knife to a chainsaw.</p>
<p>Look into Lambda or Cloud Function power tuning to counteract cold start problems - <a href="https://github.com/alexcasalboni/aws-lambda-power-tuning">AWS Lambda Power Tuning Tools Github repo</a>. Some Cloud Functions will not have any cold start problems, like the Cloudflare Workers.</p>
<p>Stick to single responsibility principle with serverless. Breaking architecture up into microservices should be always on your mind. You’ll reach a point where you need to use multiple lambda, streams (data streams) and data queues are useful for integrating micro-services and allowing for a share data flow.</p>
<p>Consider data logging and tracing early. Use AWS CloudWatch, AWS X-Ray or a 3rd party tool. It might cost a bit more, but it will save time when debugging.</p>
<p>Invest in a monitoring setup. You want to know when something gets hammered or gets throttled - a dashboard will do wonders to highlight this.</p>
<p>Take a certification course even if you don’t want to officially take the exam. The courses usually cover a lot of ground. The AWS certifications have free videos on FreeCodeCamp.</p>
<h2>Resources:</h2>
<ul>
<li>FooBar Serverless - loads of great tutorials <a href="https://www.youtube.com/channel/UCSLIvjWJwLRQze9Pn4cectQ">https://www.youtube.com/channel/UCSLIvjWJwLRQze9Pn4cectQ</a></li>
<li>Complete Coding - Sam and I worked in the same company last year and he's a great instructor, he covers a lot of serverless topics on Youtube <a href="https://www.youtube.com/channel/UC8uBP0Un18DJAnWjm1CPqBg">https://www.youtube.com/channel/UC8uBP0Un18DJAnWjm1CPqBg</a></li>
<li>Serverless Stack - the best tutorial for learning serverless, really elaborate, covers a lot of yml concepts and follows project structure best practices <a href="https://serverless-stack.com/">https://serverless-stack.com/</a></li>
<li>AWS Lambda Terraform cookbook - using Terraform to orchestrate any example of Lambda services <a href="https://github.com/nsriram/lambda-the-terraform-way">https://github.com/nsriram/lambda-the-terraform-way</a></li>
<li>FreeCodeCamp - AWS Associate Developer certification, covers a lot of serverless tools <a href="https://www.youtube.com/watch?v=RrKRN9zRBWs">https://www.youtube.com/watch?v=RrKRN9zRBWs</a></li>
<li>FreeCodeCamp - AWS Solutions Architect certification - this covers less serverless specifics, more networking, servers, business practices, can teach you a lot if you are a beginner <a href="https://www.youtube.com/watch?v=Ia-UEYYR44s&#x26;t=2616s">https://www.youtube.com/watch?v=Ia-UEYYR44s&#x26;t=2616s</a></li>
</ul>

                ]]>
            </description>
        </item><item>
            <title>Choose a Learning Path</title>
            <link>https://alanionita.github.io/portfolio--svelte/blog/2021.01.23.learning-path</link>
            <pubDate>Sat, 23 Jan 2021 00:00:00 Z</pubDate>
            <author>Alan Ionita</author>
            <description>
                <![CDATA[
                    <h1>Choose a Learning Path</h1>
<p>My interests are wide-spanning and although they reside within the broad area of technology, they usually require different skills.</p>
<p>Each interest also requires time, planning, and organisation.</p>
<p>I've been considering recently how to best optimise my learning so that I can build up momentum. After watching a video from <a href="https://www.youtube.com/watch?v=sO1ctUNQ1k8">Devslopes</a> I realised that I'm having challenges with learning because my learning is not done towards a path, a singular goal.</p>
<h2>Background</h2>
<p>Like most people I hated school. It prescribed too much of what I had to learn and didn't do a great job of relating that with why I'm learning it and how it can benefit me.</p>
<p>Most of the topics I loved were topics that had decent teachers. Those teachers were in equal parts educators and actors, they knew exactly how to spark curiosity and wonder whilst also allowing me the freedom to make my own mind on it.</p>
<p>I was glad to finish school because finally, at university, I was pursuing something I liked. In hindsight university is where you first encounter the idea of learning for a path: you enrol in a course, you set your target on an industry and you slowly find a sub-category of work that you enjoy. This freedom meant that whilst following a business degree I could also pursue learning Japanese, sociology, service design, and advertising.</p>
<p>In university I started joining different clubs and branching out my interests. This helped me avoid boredom, fight off the loneliness of being a foreign student, introduced me to new people, and helped learn some valuable life skills.</p>
<p>It did have a negative impact: I started to identify myself as a polymath.</p>
<h2>The challenge</h2>
<p>A polymath or a person with multiple interests believe that they can be good at a few things. They are 'Jack of all trades'.</p>
<p>I read once that a huge chunk of learners never become experts in a field because they become too addicted to being a beginner. As soon as they come close to mastery, they either start misjudge their experience or find new topics to learn.</p>
<p>The main issue I have is that having multiple interests, although exciting, leaves me with too much planning. If I were to focus on electronics, I could focus on this until I reach mastery.</p>
<p>Ultimately it comes down to choices and the more choices you have the higher the cost of making decision.</p>
<h2>Inspiration</h2>
<p><a href="https://www.youtube.com/watch?v=sO1ctUNQ1k8">Devslopes</a> was tackling the question 'Should I learn Python in 2021' and the conclusion was 'No' because learning Python isn't part of a path that can get a beginner into the industry.</p>
<h2>Tools and process</h2>
<p>Because of the multiple interests I've already developed some systems to manage my interests.</p>
<p>The most useful tool for validating new skills has been the 'learning radar' inspired by the <a href="https://www.thoughtworks.com/radar">Thoughworks Tech Radar</a>.</p>
<p>Every year I decide what skills, tech, tools, practices, languages &#x26; frameworks I want to hold, assess, trial, adopt.</p>
<p>For each entry in the radar I plan quarterly goals, and from there I set a mastery target. Eg. By Q1 2021 I want to get an AWS certification.</p>
<p>The quarterly goals then get slip to weekly time slots. I time-box a lot and I find it really useful when learning.</p>
<h2>Assessing the process</h2>
<p>The biggest issue with the process is that my 'learning radar' is huge, which has a knock-on effect on the amount of time I can allocate to each entry.</p>
<p>This mean that it takes me longer to reach mastery for each entry.</p>
<p>Some entries, like calculus require vastly more energy than something like web development because of the ratio of skill/complexity for that entry. I'm a lot more experienced at web development so it's easier to complete goals for that entry.</p>
<h2>Refactoring the process</h2>
<p>Started by zooming out from the 'learning radar' and considering the 'paths' of each entry.</p>
<ul>
<li>
<p>'Learning Python' aligns with 'Machine Learning Engineer' and 'Web Security Engineer' and 'Automation Engineer (Network or DevOps)' paths.</p>
</li>
<li>
<p>'Learning Rust' aligns with 'Embedded Engineer' and 'Robotics Engineer' paths.</p>
</li>
<li>
<p>'Learning Calculus' aligns with 'Machine Learning Engineer' and 'Robotics Engineer' paths.</p>
</li>
</ul>
<p>After mapping each entry to a path patterns start to develop.</p>
<p>I realise that some entries are shared by multiple paths, so I settle on the rule of only adding something new to the 'learning radar' if it aligns to 2 or more paths.</p>
<p>As hard as it is I decide to drop all other entries that don't match the above rule.</p>
<p>A small sample of learning entries I dropped:</p>
<ul>
<li>WebGL</li>
<li>Vue</li>
<li>Electron</li>
<li>Flask</li>
<li>Swift</li>
<li>Kotlin</li>
<li>Flutter</li>
</ul>
<h2>What paths am I following</h2>
<ul>
<li>Software Engineer with focus on Web and Cloud</li>
<li>Machine Learning Engineer with focus on economic models and satellite imagery</li>
<li>SysOps Engineer with a focus DevOps and Networking</li>
<li>Security Engineer with a focus on Web Security</li>
<li>Embedded Engineer with a focus on human machine interfaces and sensors</li>
<li>Robotics Engineer with a focus on autonomous systems for land / air vehicles and soft robots</li>
</ul>
<h2>Remaining work</h2>
<p>The paths themselves also need some prioritisation based on success rates. For example there's not as many 'Machine Learning Engineer' roles in my area as 'Software Engineer' roles.</p>
<p>I also need to factor in the difficulty of a particular field and how far do I want to get down that path.</p>
<p>There seem to be a lot of paths so there's more work to do here to whittle these down.</p>
<h2>Summary</h2>
<p>Don't learn anything that doesn't align to a path. I might feel nice to play around with an obscure knowledge but your time is limited.</p>
<p>Settling on a path / paths makes it easier to choose what you should learn next.</p>

                ]]>
            </description>
        </item><item>
            <title>Stack Overflow Contributions</title>
            <link>https://alanionita.github.io/portfolio--svelte/blog/2021.01.20.so-contributions</link>
            <pubDate>Wed, 20 Jan 2021 00:00:00 Z</pubDate>
            <author>Alan Ionita</author>
            <description>
                <![CDATA[
                    <h1>Stack Overflow Contributions</h1>
<p>Stack Overflow is a developers friend, but too many developers consume its content without adding to it.</p>
<p>Here is the story of what drove me to change that habit, what I'm doing now, and what results I've seen from this effort.</p>
<h2>Background</h2>
<p>I started contributing to Stack Overflow in 2018. At the time I was in my first proper development job I was the only official web developer and I had to build a lot complicated apps.</p>
<p>Stack Overflow was my oasis when I would get stuck. I also never posted questions, I was scared of looking silly, I was worried of the responses I might get, so I just consumed.</p>
<p>A few years later I started doing a small amount of open source maintaining and it massively expanded both my knowledge and self-awareness.</p>
<p>From open source I also learned how maintainers get paid nothing or poorly, even when their software is used by big businesses.</p>
<p>Open source made reassess my approach with Stack Overflow and I wanted to start contributing again.</p>
<p>To my surprise when I returned to Stack Overflow and signed in with an old account I realised that some of the questions from the start of my development journey were receiving thousands of views.</p>
<p>My early Electron questions were collectively getting 10,000 views!</p>
<h2>Epiphany</h2>
<p>I didn't want to share my questions with the Stack Overflow because I thought they were basic. My Electron posts were proof that actually I was an early adopter and as others were coming over to Electron they were encountering similar problems.</p>
<p>I was validating my ideas instead of allowing the community to choose.</p>
<h2>Analysis</h2>
<p>After the penny dropped I renamed by Stack Overflow account to match my name. I wanted to act as another element of my digital persona.</p>
<p>Now I started analysing my Stack Overflow posts and uncovered some trends:</p>
<ul>
<li>most of my posts were targeted at beginners</li>
<li>most of the posts were on new tech with growing market shares</li>
<li>some of my posts covered obscure behavior from common tasks</li>
<li>my best answers were long, simple to follow, and shared example code snippets</li>
<li>most of my questions had a lot of comments</li>
</ul>
<h2>Process</h2>
<p>From the analysis there's a clear process developing.</p>
<h3>Niche</h3>
<p>Having a niche on Stack Overflow is really important. The community mimics the tech landscape so there is a lot of topics to contribute to.</p>
<p>Each topic comes with it's own caveats too. For example, the <code>javascript</code> topic is difficult to support since there are many followers and questions get answers instantly. This makes it unlikely to provide an answer that gets voted or accepted as an answer.</p>
<p>On the other hand new technologies are barren, partly because the Venn diagram of new adopters and Stack Overflow posters is really small.</p>
<p>It's in new technologies that you have a better chance at receiving votes and getting answers accepted.</p>
<h3>Posts content and quality</h3>
<p>Stack Overflow allows you to answer your own question and I've learned early on that this system can help with my problem solving: I would find a problem, post it on Stack Overflow, and keep working on the problem. Some times I would get an answer and other times I would uncover the solution myself.</p>
<p>I found this pattern to be similar to rubber-ducking aka talking to yourself expect instead of talking to myself I'd be talking to the internet.</p>
<p>Questions on Stack Overflow have to be specific. This in itself helped my problem solving because I would usually find a solution during the question submition screen from Stack Overflow.</p>
<p>Poorly written questions get negative feedback from Stack Overflow. This can be to do with the question framing, lack of context, or a lack of difficulty. If you ask a stupid question you will know about it!</p>
<p>I realised that 'performant' questions usually are procedural: if I do X, I get Y, but I expect Z. Code samples, short copy, and useful headings make it more likely for a question to be answered or interacted with in a positive way.</p>
<p>Answers follow similar patterns: good formatting, clear headlines, and sample code. In addition to this, answers benefit from multiple scenario handling: if you want Z do this, if you use Y do this.</p>
<p>Bigger answers are RARE on Stack Overflow because the community values brevity and chases the accepted answer metric. When you write long-form answers you essentially give more context and cover more scenarios, so overall your performance is better.</p>
<p>Do notice however that this only works with niche threads, since that is where knowledge is missing. To someone learning a new technology a lengthy, in-depth explanation is just what they require.</p>
<h3>Writing style</h3>
<p>Stack Overflow has a lot of resources on <a href="https://stackoverflow.com/help/how-to-answer">how to write a good answer</a> and it is crucial guidance to follow.</p>
<p>It helped me learn how to write with a neutral tone where no one is offended by the answer and the logic of the answers is enough to avoid personal preferences or fanboy behaviour.</p>
<h3>Post frequency and lifecycle</h3>
<p>Finding a fresh pool of new questions without answers and crafting thoughtful answers takes some time so I set myself the goal to post up to 5 answers per week.</p>
<p>Both questions and answers are actually evergreen and I decided early on that there are no benefits to answering early. I wanted to produce quality content that over time would be more valuable.</p>
<h2>Results</h2>
<p>I ran the above process in August 2020 and kept up with it for two weeks.</p>
<p>At the time I was reaching 22,000 people through my questions and answers.</p>
<p>I focused mainly on the NextJS topic since it was a technology I knew and liked. The community on Stack Overflow was also lackluster even though every day there would be 10 new questions.</p>
<p>I decided to check into my account again this month - January 2021.</p>
<p>I have now reached 106,000 people!</p>
<p>For January 2021 I ranked in the top 14% percentile, with a rank of #28778.</p>
<p>I also gained +67800 change points - which outlines growth and an increase in ranking. This puts me in a group on only 5 other users.</p>
<h2>Closing remarks</h2>
<p>I'm astonished of what can be achieved with such a small amount of effort.</p>
<p>I can't believe that my content has reached 100k people and the stream of positive comments and upvotes is heart warming.</p>
<p>I wanted to expand my contribution to the platform I feel like I have achieved that goal. Little did I know that I would also improve how I write technical instructions, how to deal with online negativity, and how to problem solve.</p>
<p>I also received a book offer request through Stack Overflow.</p>

                ]]>
            </description>
        </item><item>
            <title>The MVP fallacy</title>
            <link>https://alanionita.github.io/portfolio--svelte/blog/2021.01.17.mvp-fallacy</link>
            <pubDate>Sun, 17 Jan 2021 00:00:00 Z</pubDate>
            <author>Alan Ionita</author>
            <description>
                <![CDATA[
                    <h1>The MVP fallacy</h1>
<p>Inspired by this post <a href="https://danluu.com/sounds-easy/">Sounds Easy by Dan Luu</a></p>
<blockquote>
<p>Businesses that actually care about turning a profit will spend a lot of time (hence, a lot of engineers) working on optimizing systems, even if an MVP for the system could have been built in a weekend.</p>
</blockquote>
<p>Far too often companies and especially startups prioritise speed over careful engineering. 'Proof of concepts' (POCs) and 'Minimum Viable Products' (MVPs) often become the products without the necessary work needed to make them client-ready.</p>
<p>As an R&#x26;D developer, I've seen it too often and there is no real solution to it apart from a frank discussion with those in charge.</p>
<p>MVPs allow us to prove a point, solve a problem, and then get the funding for it. That feature should not be considered 'done' until we've received the tender and redesigned the solution.</p>
<p>When it comes to R&#x26;D project I like to think of the build through the lens of <a href="https://martin.kleppmann.com/">Martin Kleppmann's </a> maintainbility principles: Evolvability, Simplicity, and Operability.</p>
<h2>Can evolve</h2>
<p>The ability of a project to be easily updated or features can easily be added. The most important build goal for R&#x26;D projects and often the one that gets neglected.</p>
<p>But should we note prioritise speed at all cost?</p>
<p>We can, but the cost of that is exponential - usually a 'act first, think later' MVP end up being rewritten. Yes it takes time to plan a solid foundation for an MVP, but the cost of that is too often discounted as wasteful. In reality a bit of planning negates a rewrite.</p>
<h2>Simplicity</h2>
<p>Keeping a project lean is actually different depending on who you ask.</p>
<p>Firstly it's important to simplify the ask from a product perspective. As developers we must realise what a user will value and incite dialogue with the product teams. A simple product is not always a simple build, but it reduces some of the 'purpose' questions that we get during a difficult build.</p>
<p>Secondly the source-code must be simple and to simplify the source-code we must abstract, organise, and make use of modules where possible.</p>
<p>MVPs are vehicles for change, they spark change or progress. Usually they trigger new features and improvements, and most of time these changes and improvements are critical. If we can already predict this then it makes sense to prioritise abstraction at build time.</p>
<h2>Can operate</h2>
<p>Making sure that the MVP can operate under different conditions.</p>
<p>It all starts with documentation and moves on to logging, monitoring, and error handling.</p>
<p>An MVP that cannot be operated is a waste of resources.</p>
<h2>Culture</h2>
<p>Culture also has a huge part to play in this MVP fallacy. When we product an MVP developers prove that something is possible and to management that also translates as 'feature X is done'.</p>
<p>The language we use contributes to this when in reality developers skirt around what we 'didn't do' to reach a deadline for the MVP.</p>
<p>This is a simple problem to solve. Developers must have the tools to explain if required for the feature to be complete and managers should never equate an MVP to a completed feature.</p>
<p>Consistently delivering MVPs that get mistaken for full features by management is a bad product scenario since the product over time starts to resemble a 'house of cards'.</p>
<h2>Summary</h2>
<p>A completed MVP should never equate a completed feature. It's important that both developers and managers agree on this.</p>
<p>Equally important is that MVP maintainability is prioritised in parallel with speed.</p>
<p>After all 'haste makes waste'.</p>

                ]]>
            </description>
        </item><item>
            <title>Lead chats or 1-2-1s</title>
            <link>https://alanionita.github.io/portfolio--svelte/blog/2021.01.17.lead-chats</link>
            <pubDate>Sun, 17 Jan 2021 00:00:00 Z</pubDate>
            <author>Alan Ionita</author>
            <description>
                <![CDATA[
                    <h1>Lead chats or 1-2-1s</h1>
<p>Having recently took on the role of tech lead for one of the squads, I wanted to document a bit about the process and system for running.</p>
<h2>People vs. Performance</h2>
<p>Manager-to-team-member chats are typically used in business for performance tracking and realignment. Most people who enter the industry have either worked in similar environments or never had the chance to experience such meetings.</p>
<p>Let me say this, 1-2-1s are not for performance! Sure performance does come up, but it should never be the goal.</p>
<p>The goals are related to personal growth and since everyone is different, every 1-2-1 is different.</p>
<p>It's also a 'process meeting', not chit-chat. As a facilitator we need to have a range of tool to overcome road block and direct the meeting towards a positive outcome.</p>
<h2>Intention and outcome</h2>
<p>Before starting a 1-2-1 I set a goal for it. I walk into a meeting or click that Meet url knowing what I want to achieve from the meeting.</p>
<p>Think hard about how the individual goals can help the team and vice versa.</p>
<p>After you've run a few meetings you'll have a list of outcomes and because we deal with human beings these outcomes end up being seasonal.</p>
<p>The ultimate goal is to help the individual reach clarity, set some short-term goals, and boots their motivation towards their new goals.</p>
<h2>Prep</h2>
<p>Review the persons past performance and make a note of feedback you want to share. This is an editing process and depends on the individual and their current 'season' - if someone is already unmotivated then withhold the difficult feedback.</p>
<p>Review the individuals previous goals. Have they made progress? Have they completed the goal? Did they receive any feedback about the most recent goal?</p>
<p>I also have a few canned questions to help drive the conversation:</p>
<blockquote>
<p>What have you enjoyed the most about the past sprint? Have you been having fun lately?</p>
</blockquote>
<p>Fun is crucial to fulfilling work and the question helps us focus on the positive parts of the sprint.</p>
<blockquote>
<p>What feedback do you have for me?</p>
</blockquote>
<p>This is a tricky question since everyone struggles to give feedback on the spot, but I repeat this each week. Usually I don't receive feedback, but when there's a bad week I always know about it.</p>
<p>This question is also trust building. I ask it every week because I care about feedback and I want my team to know that. I also want them to know that we all make mistakes and that I really care about knowing when I've made mistakes.</p>
<blockquote>
<p>How are you feeling after last sprint?</p>
</blockquote>
<p>Simple question to put everyone at ease and have a bit of a laugh. This is a great question to warm people up to talking about home life or in general discuss current emotions.</p>
<h2>During</h2>
<p>Go where the conversation goes until you reach clarity. Once you have clarity state it, filter it together, and reach an agreement. Once you've agreed, choose another questions from above or continue.</p>
<blockquote>
<p>Here is some feedback... What do you think of it?</p>
</blockquote>
<p>Provide the feedback to the team member and ask for their opinion. I think of feedback as a tool for growth, so usually framed as such and always delivered positively.</p>
<p>I tend to think of feedback of 'High five' and 'Watch out'.</p>
<p>Link 'High five' feedback to the receivers skills or ways of working.</p>
<p>When giving out 'Watch out' feedback make sure you also offer a clear path for adjustment and focus on habits and systems that can improve the situation.</p>
<p>Adjust you delivery to the receiver: some people are open to chat whilst other requires some coaxing.</p>
<p>Remember to have fun. There's a lot to think about, but were not robots - enjoy the conversation and company more than aiming to tick all the boxes.</p>
<h2>Habits</h2>
<p>Plan your meetings in advance and book the time in your calendar.</p>
<p>Dedicate a specific time for the meetings, I find that 25 minutes is enough to get through most of the above. If a wider discussion is needed I'll book a follow-up.</p>
<p>Try to batch your meetings together, it stops you from switching context. These meetings use up a lot of energy so its better to focus on them before you jump on the next tech task.</p>
<h2>Other tools</h2>
<p>Since we're talking about individual growth, it's important to try and build other tools that can help you maintain individual growth.</p>
<p>One easy tool is a share scoring system. The system is only used for self-reflection is particularly useful for developers who can benchmark their technical and soft skills.</p>
<p>Have the company policy on hand for those times when you get company policy questions.</p>
<p>Build up a collection of useful resources and work through that as a team. This helps with clarity and lets everyone self-serve when they want to level up.</p>

                ]]>
            </description>
        </item><item>
            <title>Why write a blog?</title>
            <link>https://alanionita.github.io/portfolio--svelte/blog/2021.01.13.why-write</link>
            <pubDate>Wed, 13 Jan 2021 00:00:00 Z</pubDate>
            <author>Alan Ionita</author>
            <description>
                <![CDATA[
                    <h1>Why write a blog?</h1>
<p>Selfish reasons - I often use writing for work and often repeat why and how I approach things.</p>
<p>This blog post is for me and other who want to understand the same things I care about: tech, social justice, learning, and more.</p>
<h2>Writing and I</h2>
<p>In school, some 12 years ago, I had to pick between maths and literature. It was a mandatory choice and considering my poor maths performance I picked literature.</p>
<p>I was fortunate to be taught by an amazing teacher and what started as trepidation ended up developing into a passion.</p>
<p>Literature and software development is not that different - they both involve a lot of writing, editing, and conceptualising ideas. The both require analysis and creativity and the both feel like introspective pursuits even though each profession has become more and more social since I first encountered them.</p>
<p>Why didn't I do programming instead of literature? I never had the option, nor the equipment. Before my first PC I used computers in school for learning Pascal and Word or in internet-caffes to shoot my friends in Counter Strike or race against them in Need For Speed.</p>
<h2>Purpose and content</h2>
<p>This is intended to be a raw collection of thoughts and ideas. Like my interests, this blog with snake and weave through a lot of topics.</p>

                ]]>
            </description>
        </item>
            </channel>
        </rss>